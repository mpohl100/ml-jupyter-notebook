{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5baa1a4e",
   "metadata": {},
   "source": [
    "# MNIST Dataset Processing and Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a41b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 00:11:12.907197: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-28 00:11:12.909323: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-28 00:11:12.918471: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-28 00:11:12.944137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735344672.980653    1820 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735344672.991354    1820 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-28 00:11:13.033352: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7a49b",
   "metadata": {},
   "source": [
    "## Step 1: Download and Process MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a423b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step \n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Resize images to 16x16 (256 pixels)\n",
    "x_train = x_train[:, ::2, ::2]  # Downsample by skipping every other pixel\n",
    "x_test = x_test[:, ::2, ::2]\n",
    "\n",
    "# Normalize pixel values to range [0.0, 1.0] (0.0 for white, 1.0 for black)\n",
    "x_train = 1 - x_train / 255.0\n",
    "x_test = 1 - x_test / 255.0\n",
    "\n",
    "# Flatten images into 256-pixel vectors\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train, 10)\n",
    "y_test_onehot = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed3dd7",
   "metadata": {},
   "source": [
    "## Step 2: Save Processed Data to CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acefa21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved input.csv and target.csv.\n"
     ]
    }
   ],
   "source": [
    "# Save input data to input.csv\n",
    "input_df = pd.DataFrame(x_train_flat)\n",
    "input_df.to_csv(\"input.csv\", index=False, header=False)\n",
    "\n",
    "\n",
    "# Save target data to target.csv\n",
    "target_df = pd.DataFrame(y_train_onehot)\n",
    "target_df.to_csv(\"target.csv\", index=False, header=False)\n",
    "\n",
    "print(\"Saved input.csv and target.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed74097",
   "metadata": {},
   "source": [
    "## Step 2.1 Train neural network based on shape file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 196 }, activation: ActivationData { activation_type: Sigmoid, temperature: None } }, LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: ActivationData { activation_type: Sigmoid, temperature: None } }] }\n",
      "Epoch: 0\n",
      "Epoch 0: Loss 0.2994335927243328, Accuracy 41.524798209481176%\n",
      "Epoch: 1\n",
      "Epoch 1: Loss 0.19872751729186489, Accuracy 59.596657063263414%\n",
      "Epoch: 2\n",
      "Epoch 2: Loss 0.18270421560976544, Accuracy 63.04912021714802%\n",
      "Epoch: 3\n",
      "Epoch 3: Loss 0.172847491382305, Accuracy 65.08012095526084%\n",
      "Epoch: 4\n",
      "Epoch 4: Loss 0.16602366551452546, Accuracy 66.66825400604776%\n",
      "Epoch: 5\n",
      "Epoch 5: Loss 0.16078704062987711, Accuracy 67.97542798638064%\n",
      "Epoch: 6\n",
      "Epoch 6: Loss 0.1574147647173271, Accuracy 68.59687135407985%\n",
      "Epoch: 7\n",
      "Epoch 7: Loss 0.1556914232610189, Accuracy 68.69449272601729%\n",
      "Epoch: 8\n",
      "Epoch 8: Loss 0.15480812929353077, Accuracy 68.55401319079026%\n",
      "Epoch: 9\n",
      "Epoch 9: Loss 0.15474201731893916, Accuracy 68.37067549227362%\n",
      "Epoch: 10\n",
      "Epoch 10: Loss 0.15466630006582494, Accuracy 68.48972594585585%\n",
      "Epoch: 11\n",
      "Epoch 11: Loss 0.1576048504459649, Accuracy 67.38969975475607%\n",
      "Epoch: 12\n",
      "Epoch 12: Loss 0.16032074850532618, Accuracy 67.27779232838877%\n",
      "Epoch: 13\n",
      "Epoch 13: Loss 0.15927200931633814, Accuracy 67.32541250982166%\n",
      "Epoch: 14\n",
      "Epoch 14: Loss 0.1620267253980221, Accuracy 66.17062311007405%\n",
      "Epoch: 15\n",
      "Epoch 15: Loss 0.162342255095014, Accuracy 66.78492345055834%\n",
      "Epoch: 16\n",
      "Epoch 16: Loss 0.1623767858593231, Accuracy 66.51825043453415%\n",
      "Epoch: 17\n",
      "Epoch 17: Loss 0.16128799741118677, Accuracy 66.77301840520012%\n",
      "Epoch: 18\n",
      "Epoch 18: Loss 0.16159729556377672, Accuracy 66.99683325793472%\n",
      "Epoch: 19\n",
      "Epoch 19: Loss 0.1623990019057209, Accuracy 66.56587061596704%\n",
      "Epoch: 20\n",
      "Epoch 20: Loss 0.16307601666473523, Accuracy 66.66111097883282%\n",
      "Epoch: 21\n",
      "Epoch 21: Loss 0.16259423382511304, Accuracy 66.74444629634039%\n",
      "Epoch: 22\n",
      "Epoch 22: Loss 0.1627188513161814, Accuracy 66.72777923283888%\n",
      "Epoch: 23\n",
      "Epoch 23: Loss 0.16832675876723172, Accuracy 64.98249958332342%\n",
      "Epoch: 24\n",
      "Epoch 24: Loss 0.16635099882677157, Accuracy 66.66587299697612%\n",
      "Epoch: 25\n",
      "Epoch 25: Loss 0.18590034949690784, Accuracy 60.47524941070026%\n",
      "Epoch: 26\n",
      "Epoch 26: Loss 0.22315669218887013, Accuracy 53.022691016452775%\n",
      "Epoch: 27\n",
      "Epoch 27: Loss 0.3052001800142881, Accuracy 38.665206314436055%\n",
      "Epoch: 28\n",
      "Epoch 28: Loss 0.3112741922338995, Accuracy 39.2390295007024%\n",
      "Epoch: 29\n",
      "Epoch 29: Loss 0.29117020786266823, Accuracy 43.13674135098455%\n",
      "Epoch: 30\n",
      "Epoch 30: Loss 0.2721734116868959, Accuracy 46.14871782661492%\n",
      "Epoch: 31\n",
      "Epoch 31: Loss 0.3324742894607222, Accuracy 35.29845948713064%\n",
      "Epoch: 32\n",
      "Epoch 32: Loss 0.4775814258253589, Accuracy 22.57910902640539%\n",
      "Epoch: 33\n",
      "Epoch 33: Loss 0.7913301148583752, Accuracy 2.566727779232839%\n",
      "Epoch: 34\n",
      "Epoch 34: Loss 0.8702706494594459, Accuracy 0.09524036286578252%\n",
      "Epoch: 35\n",
      "Epoch 35: Loss 0.8778208031306998, Accuracy 0.002381009071644563%\n",
      "Epoch: 36\n",
      "Epoch 36: Loss 0.8828833982083923, Accuracy 0%\n",
      "Epoch: 37\n",
      "Epoch 37: Loss 0.8849123668401314, Accuracy 0%\n",
      "Epoch: 38\n",
      "Epoch 38: Loss 0.8863719484849852, Accuracy 0%\n",
      "Epoch: 39\n",
      "Epoch 39: Loss 0.8876566949666358, Accuracy 0%\n",
      "Epoch: 40\n",
      "Epoch 40: Loss 0.8884158210931611, Accuracy 0%\n",
      "Epoch: 41\n",
      "Epoch 41: Loss 0.8890544505805502, Accuracy 0%\n",
      "Epoch: 42\n",
      "Epoch 42: Loss 0.8895290218435515, Accuracy 0%\n",
      "Epoch: 43\n",
      "Epoch 43: Loss 0.8898867986101137, Accuracy 0%\n",
      "Epoch: 44\n",
      "Epoch 44: Loss 0.890198206529557, Accuracy 0%\n",
      "Epoch: 45\n",
      "Epoch 45: Loss 0.8902868392407071, Accuracy 0%\n",
      "Epoch: 46\n",
      "Epoch 46: Loss 0.8902274733860308, Accuracy 0%\n",
      "Epoch: 47\n",
      "Epoch 47: Loss 0.8907353558667739, Accuracy 0%\n",
      "Epoch: 48\n",
      "Epoch 48: Loss 0.8911966857631584, Accuracy 0%\n",
      "Epoch: 49\n",
      "Epoch 49: Loss 0.8914078797949614, Accuracy 0%\n",
      "Epoch: 50\n",
      "Epoch 50: Loss 0.8916507164510751, Accuracy 0%\n",
      "Epoch: 51\n",
      "Epoch 51: Loss 0.8918177438238526, Accuracy 0%\n",
      "Epoch: 52\n",
      "Epoch 52: Loss 0.8918267420920734, Accuracy 0%\n",
      "Epoch: 53\n",
      "Epoch 53: Loss 0.892033849120324, Accuracy 0%\n",
      "Epoch: 54\n",
      "Epoch 54: Loss 0.8922810680562535, Accuracy 0%\n",
      "Epoch: 55\n",
      "Epoch 55: Loss 0.8922281899719252, Accuracy 0%\n",
      "Epoch: 56\n",
      "Epoch 56: Loss 0.8922976771335586, Accuracy 0%\n",
      "Epoch: 57\n",
      "Epoch 57: Loss 0.8925879581690239, Accuracy 0%\n",
      "Epoch: 58\n",
      "Epoch 58: Loss 0.8926930041857866, Accuracy 0%\n",
      "Epoch: 59\n",
      "Epoch 59: Loss 0.8927663657142061, Accuracy 0%\n",
      "Epoch: 60\n",
      "Epoch 60: Loss 0.8928238367246083, Accuracy 0%\n",
      "Epoch: 61\n",
      "Epoch 61: Loss 0.8928465714007792, Accuracy 0%\n",
      "Epoch: 62\n",
      "Epoch 62: Loss 0.8929065459762072, Accuracy 0%\n",
      "Epoch: 63\n",
      "Epoch 63: Loss 0.8929499189747324, Accuracy 0%\n",
      "Epoch: 64\n",
      "Epoch 64: Loss 0.8929824112668799, Accuracy 0%\n",
      "Epoch: 65\n",
      "Epoch 65: Loss 0.8929819846743127, Accuracy 0%\n",
      "Epoch: 66\n",
      "Epoch 66: Loss 0.8929869775807227, Accuracy 0%\n",
      "Epoch: 67\n",
      "Epoch 67: Loss 0.8930395801136981, Accuracy 0%\n",
      "Epoch: 68\n",
      "Epoch 68: Loss 0.8931202696595467, Accuracy 0%\n",
      "Epoch: 69\n",
      "Epoch 69: Loss 0.8931783415020176, Accuracy 0%\n",
      "Epoch: 70\n",
      "Epoch 70: Loss 0.8932005729116771, Accuracy 0%\n",
      "Epoch: 71\n",
      "Epoch 71: Loss 0.8932079751534386, Accuracy 0%\n",
      "Epoch: 72\n",
      "Epoch 72: Loss 0.8932189104333401, Accuracy 0%\n",
      "Epoch: 73\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define parameters for `nn_generator`\n",
    "model_directory = \"./trained_model\"\n",
    "# create the trained model directory\n",
    "import os\n",
    "os.makedirs(model_directory, exist_ok=True)\n",
    "\n",
    "input_file = \"input.csv\"\n",
    "target_file = \"target.csv\"\n",
    "shape_file = \"nn_shape.yaml\"\n",
    "training_verification_ratio = 0.7\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "tolerance = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "# Create the command for training the neural network\n",
    "command = [\n",
    "    \"./train\",\n",
    "    \"--model-directory\", model_directory,\n",
    "    \"--input-file\", input_file,\n",
    "    \"--target-file\", target_file,\n",
    "    \"--shape-file\", shape_file,\n",
    "    \"--training-verification-ratio\", str(training_verification_ratio),\n",
    "    \"--learning-rate\", str(learning_rate),\n",
    "    \"--epochs\", str(epochs),\n",
    "    \"--tolerance\", str(tolerance),\n",
    "    \"--batch-size\", str(batch_size),\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "try:\n",
    "    #subprocess.run(command, check=True)\n",
    "    # Create a copy of the current environment variables and add RUST_BACKTRACE=1\n",
    "    env = os.environ.copy()\n",
    "    env[\"RUST_BACKTRACE\"] = \"1\"\n",
    "\n",
    "    # Run the command\n",
    "    subprocess.run(command, check=True, env=env)\n",
    "    print(\"Model training completed successfully and saved in:\", model_directory)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error occurred during model training:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f708ac4e",
   "metadata": {},
   "source": [
    "## Step 3: Create a Neural Network using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb6f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: ./trained_model with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 758 }, activation: Tanh }, LayerShape { layer_type: Dense { input_size: 758, output_size: 196 }, activation: ReLU }, LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Epoch 0: Loss 0.32277414660446563\n",
      "Epoch: 1\n",
      "Epoch 0: Loss 0.32277414660446563\n",
      "Epoch: 1\n",
      "Epoch 0: Loss 0.32277414660446563\n",
      "Epoch: 1\n",
      "Epoch 1: Loss 0.26502724016561197\n",
      "Epoch: 2\n",
      "Epoch 1: Loss 0.26502724016561197\n",
      "Epoch: 2\n",
      "Epoch 1: Loss 0.26502724016561197\n",
      "Epoch: 2\n",
      "Epoch 2: Loss 0.25539231975668025\n",
      "Epoch: 3\n",
      "Epoch 2: Loss 0.25539231975668025\n",
      "Epoch: 3\n",
      "Epoch 2: Loss 0.25539231975668025\n",
      "Epoch: 3\n",
      "Epoch 3: Loss 0.2502114973723556\n",
      "Epoch: 4\n",
      "Epoch 3: Loss 0.2502114973723556\n",
      "Epoch: 4\n",
      "Epoch 3: Loss 0.2502114973723556\n",
      "Epoch: 4\n",
      "Epoch 4: Loss 0.24681649989584786\n",
      "Epoch: 5\n",
      "Epoch 4: Loss 0.24681649989584786\n",
      "Epoch: 5\n",
      "Epoch 4: Loss 0.24681649989584786\n",
      "Epoch: 5\n",
      "Epoch 5: Loss 0.24437722125477435\n",
      "Epoch: 6\n",
      "Epoch 5: Loss 0.24437722125477435\n",
      "Epoch: 6\n",
      "Epoch 5: Loss 0.24437722125477435\n",
      "Epoch: 6\n",
      "Epoch 6: Loss 0.24250877002618654\n",
      "Epoch: 7\n",
      "Epoch 6: Loss 0.24250877002618654\n",
      "Epoch: 7\n",
      "Epoch 6: Loss 0.24250877002618654\n",
      "Epoch: 7\n",
      "Epoch 7: Loss 0.24102103770605712\n",
      "Epoch: 8\n",
      "Epoch 7: Loss 0.24102103770605712\n",
      "Epoch: 8\n",
      "Epoch 7: Loss 0.24102103770605712\n",
      "Epoch: 8\n",
      "Epoch 8: Loss 0.23980837425952023\n",
      "Epoch: 9\n",
      "Epoch 8: Loss 0.23980837425952023\n",
      "Epoch: 9\n",
      "Epoch 8: Loss 0.23980837425952023\n",
      "Epoch: 9\n",
      "Epoch 9: Loss 0.23880081195432196\n",
      "Epoch: 10\n",
      "Epoch 9: Loss 0.23880081195432196\n",
      "Epoch: 10\n",
      "Epoch 9: Loss 0.23880081195432196\n",
      "Epoch: 10\n",
      "Epoch 10: Loss 0.23794887314106106\n",
      "Epoch: 11\n",
      "Epoch 10: Loss 0.23794887314106106\n",
      "Epoch: 11\n",
      "Epoch 10: Loss 0.23794887314106106\n",
      "Epoch: 11\n",
      "Epoch 11: Loss 0.2372165979629189\n",
      "Epoch: 12\n",
      "Epoch 11: Loss 0.2372165979629189\n",
      "Epoch: 12\n",
      "Epoch 11: Loss 0.2372165979629189\n",
      "Epoch: 12\n",
      "Epoch 12: Loss 0.2365774873838539\n",
      "Epoch: 13\n",
      "Epoch 12: Loss 0.2365774873838539\n",
      "Epoch: 13\n",
      "Epoch 12: Loss 0.2365774873838539\n",
      "Epoch: 13\n",
      "Epoch 13: Loss 0.2360119464004573\n",
      "Epoch: 14\n",
      "Epoch 13: Loss 0.2360119464004573\n",
      "Epoch: 14\n",
      "Epoch 13: Loss 0.2360119464004573\n",
      "Epoch: 14\n",
      "Epoch 14: Loss 0.23550630062490882\n",
      "Epoch: 15\n",
      "Epoch 14: Loss 0.23550630062490882\n",
      "Epoch: 15\n",
      "Epoch 14: Loss 0.23550630062490882\n",
      "Epoch: 15\n",
      "Epoch 15: Loss 0.23505127743678486\n",
      "Epoch: 16\n",
      "Epoch 15: Loss 0.23505127743678486\n",
      "Epoch: 16\n",
      "Epoch 15: Loss 0.23505127743678486\n",
      "Epoch: 16\n",
      "Epoch 16: Loss 0.2346399409357362\n",
      "Epoch: 17\n",
      "Epoch 16: Loss 0.2346399409357362\n",
      "Epoch: 17\n",
      "Epoch 16: Loss 0.2346399409357362\n",
      "Epoch: 17\n",
      "Epoch 17: Loss 0.23426634587791317\n",
      "Epoch: 18\n",
      "Epoch 17: Loss 0.23426634587791317\n",
      "Epoch: 18\n",
      "Epoch 17: Loss 0.23426634587791317\n",
      "Epoch: 18\n",
      "Epoch 18: Loss 0.23392516915498182\n",
      "Epoch: 19\n",
      "Epoch 18: Loss 0.23392516915498182\n",
      "Epoch: 19\n",
      "Epoch 18: Loss 0.23392516915498182\n",
      "Epoch: 19\n",
      "Epoch 19: Loss 0.23361180004190338\n",
      "Epoch: 20\n",
      "Epoch 19: Loss 0.23361180004190338\n",
      "Epoch: 20\n",
      "Epoch 19: Loss 0.23361180004190338\n",
      "Epoch: 20\n",
      "Epoch 20: Loss 0.23332241542927085\n",
      "Epoch: 21\n",
      "Epoch 20: Loss 0.23332241542927085\n",
      "Epoch: 21\n",
      "Epoch 20: Loss 0.23332241542927085\n",
      "Epoch: 21\n",
      "Epoch 21: Loss 0.23305392559360416\n",
      "Epoch: 22\n",
      "Epoch 21: Loss 0.23305392559360416\n",
      "Epoch: 22\n",
      "Epoch 21: Loss 0.23305392559360416\n",
      "Epoch: 22\n",
      "Epoch 22: Loss 0.23280384978276558\n",
      "Epoch: 23\n",
      "Epoch 22: Loss 0.23280384978276558\n",
      "Epoch: 23\n",
      "Epoch 22: Loss 0.23280384978276558\n",
      "Epoch: 23\n",
      "Epoch 23: Loss 0.232570183046648\n",
      "Epoch: 24\n",
      "Epoch 23: Loss 0.232570183046648\n",
      "Epoch: 24\n",
      "Epoch 23: Loss 0.232570183046648\n",
      "Epoch: 24\n",
      "Epoch 24: Loss 0.23235128194902288\n",
      "Epoch: 25\n",
      "Epoch 24: Loss 0.23235128194902288\n",
      "Epoch: 25\n",
      "Epoch 24: Loss 0.23235128194902288\n",
      "Epoch: 25\n",
      "Epoch 25: Loss 0.23214577623746074\n",
      "Epoch: 26\n",
      "Epoch 25: Loss 0.23214577623746074\n",
      "Epoch: 26\n",
      "Epoch 25: Loss 0.23214577623746074\n",
      "Epoch: 26\n",
      "Epoch 26: Loss 0.2319525038040332\n",
      "Epoch: 27\n",
      "Epoch 26: Loss 0.2319525038040332\n",
      "Epoch: 27\n",
      "Epoch 26: Loss 0.2319525038040332\n",
      "Epoch: 27\n",
      "Epoch 27: Loss 0.23177046240178667\n",
      "Epoch: 28\n",
      "Epoch 27: Loss 0.23177046240178667\n",
      "Epoch: 28\n",
      "Epoch 27: Loss 0.23177046240178667\n",
      "Epoch: 28\n",
      "Epoch 28: Loss 0.2315987725179126\n",
      "Epoch: 29\n",
      "Epoch 28: Loss 0.2315987725179126\n",
      "Epoch: 29\n",
      "Epoch 28: Loss 0.2315987725179126\n",
      "Epoch: 29\n",
      "Epoch 29: Loss 0.23143664848110868\n",
      "Epoch: 30\n",
      "Epoch 29: Loss 0.23143664848110868\n",
      "Epoch: 30\n",
      "Epoch 29: Loss 0.23143664848110868\n",
      "Epoch: 30\n",
      "Epoch 30: Loss 0.23128337649983138\n",
      "Epoch: 31\n",
      "Epoch 30: Loss 0.23128337649983138\n",
      "Epoch: 31\n",
      "Epoch 30: Loss 0.23128337649983138\n",
      "Epoch: 31\n",
      "Epoch 31: Loss 0.2311382987120927\n",
      "Epoch: 32\n",
      "Epoch 31: Loss 0.2311382987120927\n",
      "Epoch: 32\n",
      "Epoch 31: Loss 0.2311382987120927\n",
      "Epoch: 32\n",
      "Epoch 32: Loss 0.23100080236562684\n",
      "Epoch: 33\n",
      "Epoch 32: Loss 0.23100080236562684\n",
      "Epoch: 33\n",
      "Epoch 32: Loss 0.23100080236562684\n",
      "Epoch: 33\n",
      "Epoch 33: Loss 0.2308703133103028\n",
      "Epoch: 34\n",
      "Epoch 33: Loss 0.2308703133103028\n",
      "Epoch: 34\n",
      "Epoch 33: Loss 0.2308703133103028\n",
      "Epoch: 34\n",
      "Epoch 34: Loss 0.2307462929759982\n",
      "Epoch: 35\n",
      "Epoch 34: Loss 0.2307462929759982\n",
      "Epoch: 35\n",
      "Epoch 34: Loss 0.2307462929759982\n",
      "Epoch: 35\n",
      "Epoch 35: Loss 0.23062823786661713\n",
      "Epoch: 36\n",
      "Epoch 35: Loss 0.23062823786661713\n",
      "Epoch: 36\n",
      "Epoch 35: Loss 0.23062823786661713\n",
      "Epoch: 36\n",
      "Epoch 36: Loss 0.23051568043403395\n",
      "Epoch: 37\n",
      "Epoch 36: Loss 0.23051568043403395\n",
      "Epoch: 37\n",
      "Epoch 36: Loss 0.23051568043403395\n",
      "Epoch: 37\n",
      "Epoch 37: Loss 0.23040819015708056\n",
      "Epoch: 38\n",
      "Epoch 37: Loss 0.23040819015708056\n",
      "Epoch: 38\n",
      "Epoch 37: Loss 0.23040819015708056\n",
      "Epoch: 38\n",
      "Epoch 38: Loss 0.23030537384603184\n",
      "Epoch: 39\n",
      "Epoch 38: Loss 0.23030537384603184\n",
      "Epoch: 39\n",
      "Epoch 38: Loss 0.23030537384603184\n",
      "Epoch: 39\n",
      "Epoch 39: Loss 0.23020687462989714\n",
      "Epoch: 40\n",
      "Epoch 39: Loss 0.23020687462989714\n",
      "Epoch: 40\n",
      "Epoch 39: Loss 0.23020687462989714\n",
      "Epoch: 40\n",
      "Epoch 40: Loss 0.23011236965856513\n",
      "Epoch: 41\n",
      "Epoch 40: Loss 0.23011236965856513\n",
      "Epoch: 41\n",
      "Epoch 40: Loss 0.23011236965856513\n",
      "Epoch: 41\n",
      "Epoch 41: Loss 0.23002156704824023\n",
      "Epoch: 42\n",
      "Epoch 41: Loss 0.23002156704824023\n",
      "Epoch: 42\n",
      "Epoch 41: Loss 0.23002156704824023\n",
      "Epoch: 42\n",
      "Epoch 42: Loss 0.22993420278891388\n",
      "Epoch: 43\n",
      "Epoch 42: Loss 0.22993420278891388\n",
      "Epoch: 43\n",
      "Epoch 42: Loss 0.22993420278891388\n",
      "Epoch: 43\n",
      "Epoch 43: Loss 0.22985003814503607\n",
      "Epoch: 44\n",
      "Epoch 43: Loss 0.22985003814503607\n",
      "Epoch: 44\n",
      "Epoch 43: Loss 0.22985003814503607\n",
      "Epoch: 44\n",
      "Epoch 44: Loss 0.22976885767656183\n",
      "Epoch: 45\n",
      "Epoch 44: Loss 0.22976885767656183\n",
      "Epoch: 45\n",
      "Epoch 44: Loss 0.22976885767656183\n",
      "Epoch: 45\n",
      "Epoch 45: Loss 0.22969046766290863\n",
      "Epoch: 46\n",
      "Epoch 45: Loss 0.22969046766290863\n",
      "Epoch: 46\n",
      "Epoch 45: Loss 0.22969046766290863\n",
      "Epoch: 46\n",
      "Epoch 46: Loss 0.22961469460255834\n",
      "Epoch: 47\n",
      "Epoch 46: Loss 0.22961469460255834\n",
      "Epoch: 47\n",
      "Epoch 46: Loss 0.22961469460255834\n",
      "Epoch: 47\n",
      "Epoch 47: Loss 0.2295413835647168\n",
      "Epoch: 48\n",
      "Epoch 47: Loss 0.2295413835647168\n",
      "Epoch: 48\n",
      "Epoch 47: Loss 0.2295413835647168\n",
      "Epoch: 48\n",
      "Epoch 48: Loss 0.22947039634538766\n",
      "Epoch: 49\n",
      "Epoch 48: Loss 0.22947039634538766\n",
      "Epoch: 49\n",
      "Epoch 48: Loss 0.22947039634538766\n",
      "Epoch: 49\n",
      "Epoch 49: Loss 0.22940160950983216\n",
      "Epoch: 50\n",
      "Epoch 49: Loss 0.22940160950983216\n",
      "Epoch: 50\n",
      "Epoch 49: Loss 0.22940160950983216\n",
      "Epoch: 50\n",
      "Epoch 50: Loss 0.22933491244697937\n",
      "Epoch: 51\n",
      "Epoch 50: Loss 0.22933491244697937\n",
      "Epoch: 51\n",
      "Epoch 50: Loss 0.22933491244697937\n",
      "Epoch: 51\n",
      "Epoch 51: Loss 0.22927020554183214\n",
      "Epoch: 52\n",
      "Epoch 51: Loss 0.22927020554183214\n",
      "Epoch: 52\n",
      "Epoch 51: Loss 0.22927020554183214\n",
      "Epoch: 52\n",
      "Epoch 52: Loss 0.22920739852771613\n",
      "Epoch: 53\n",
      "Epoch 52: Loss 0.22920739852771613\n",
      "Epoch: 53\n",
      "Epoch 52: Loss 0.22920739852771613\n",
      "Epoch: 53\n",
      "Epoch 53: Loss 0.22914640903845843\n",
      "Epoch: 54\n",
      "Epoch 53: Loss 0.22914640903845843\n",
      "Epoch: 54\n",
      "Epoch 53: Loss 0.22914640903845843\n",
      "Epoch: 54\n",
      "Epoch 54: Loss 0.22908716135324328\n",
      "Epoch: 55\n",
      "Epoch 54: Loss 0.22908716135324328\n",
      "Epoch: 55\n",
      "Epoch 54: Loss 0.22908716135324328\n",
      "Epoch: 55\n",
      "Epoch 55: Loss 0.2290295853138948\n",
      "Epoch: 56\n",
      "Epoch 55: Loss 0.2290295853138948\n",
      "Epoch: 56\n",
      "Epoch 55: Loss 0.2290295853138948\n",
      "Epoch: 56\n",
      "Epoch 56: Loss 0.22897361539242939\n",
      "Epoch: 57\n",
      "Epoch 56: Loss 0.22897361539242939\n",
      "Epoch: 57\n",
      "Epoch 56: Loss 0.22897361539242939\n",
      "Epoch: 57\n",
      "Epoch 57: Loss 0.2289191898902632\n",
      "Epoch: 58\n",
      "Epoch 57: Loss 0.2289191898902632\n",
      "Epoch: 58\n",
      "Epoch 57: Loss 0.2289191898902632\n",
      "Epoch: 58\n",
      "Epoch 58: Loss 0.22886625025658222\n",
      "Epoch: 59\n",
      "Epoch 58: Loss 0.22886625025658222\n",
      "Epoch: 59\n",
      "Epoch 58: Loss 0.22886625025658222\n",
      "Epoch: 59\n",
      "Epoch 59: Loss 0.22881474051905262\n",
      "Epoch: 60\n",
      "Epoch 59: Loss 0.22881474051905262\n",
      "Epoch: 60\n",
      "Epoch 59: Loss 0.22881474051905262\n",
      "Epoch: 60\n",
      "Epoch 60: Loss 0.22876460682374364\n",
      "Epoch: 61\n",
      "Epoch 60: Loss 0.22876460682374364\n",
      "Epoch: 61\n",
      "Epoch 60: Loss 0.22876460682374364\n",
      "Epoch: 61\n",
      "Epoch 61: Loss 0.22871579708223141\n",
      "Epoch: 62\n",
      "Epoch 61: Loss 0.22871579708223141\n",
      "Epoch: 62\n",
      "Epoch 61: Loss 0.22871579708223141\n",
      "Epoch: 62\n",
      "Epoch 62: Loss 0.22866826072224636\n",
      "Epoch: 63\n",
      "Epoch 62: Loss 0.22866826072224636\n",
      "Epoch: 63\n",
      "Epoch 62: Loss 0.22866826072224636\n",
      "Epoch: 63\n",
      "Epoch 63: Loss 0.22862194853464754\n",
      "Epoch: 64\n",
      "Epoch 63: Loss 0.22862194853464754\n",
      "Epoch: 64\n",
      "Epoch 63: Loss 0.22862194853464754\n",
      "Epoch: 64\n",
      "Epoch 64: Loss 0.2285768126047131\n",
      "Epoch: 65\n",
      "Epoch 64: Loss 0.2285768126047131\n",
      "Epoch: 65\n",
      "Epoch 64: Loss 0.2285768126047131\n",
      "Epoch: 65\n",
      "Epoch 65: Loss 0.22853280631182252\n",
      "Epoch: 66\n",
      "Epoch 65: Loss 0.22853280631182252\n",
      "Epoch: 66\n",
      "Epoch 65: Loss 0.22853280631182252\n",
      "Epoch: 66\n",
      "Epoch 66: Loss 0.22848988437876902\n",
      "Epoch: 67\n",
      "Epoch 66: Loss 0.22848988437876902\n",
      "Epoch: 67\n",
      "Epoch 66: Loss 0.22848988437876902\n",
      "Epoch: 67\n",
      "Epoch 67: Loss 0.2284480029511711\n",
      "Epoch: 68\n",
      "Epoch 67: Loss 0.2284480029511711\n",
      "Epoch: 68\n",
      "Epoch 67: Loss 0.2284480029511711\n",
      "Epoch: 68\n",
      "Epoch 68: Loss 0.22840711968885288\n",
      "Epoch: 69\n",
      "Epoch 68: Loss 0.22840711968885288\n",
      "Epoch: 69\n",
      "Epoch 68: Loss 0.22840711968885288\n",
      "Epoch: 69\n",
      "Epoch 69: Loss 0.22836719385371282\n",
      "Epoch: 70\n",
      "Epoch 69: Loss 0.22836719385371282\n",
      "Epoch: 70\n",
      "Epoch 69: Loss 0.22836719385371282\n",
      "Epoch: 70\n",
      "Epoch 70: Loss 0.22832818638244134\n",
      "Epoch: 71\n",
      "Epoch 70: Loss 0.22832818638244134\n",
      "Epoch: 71\n",
      "Epoch 70: Loss 0.22832818638244134\n",
      "Epoch: 71\n",
      "Epoch 71: Loss 0.22829005993626317\n",
      "Epoch: 72\n",
      "Epoch 71: Loss 0.22829005993626317\n",
      "Epoch: 72\n",
      "Epoch 71: Loss 0.22829005993626317\n",
      "Epoch: 72\n",
      "Epoch 72: Loss 0.22825277892388332\n",
      "Epoch: 73\n",
      "Epoch 72: Loss 0.22825277892388332\n",
      "Epoch: 73\n",
      "Epoch 72: Loss 0.22825277892388332\n",
      "Epoch: 73\n",
      "Epoch 73: Loss 0.22821630949662058\n",
      "Epoch: 74\n",
      "Epoch 73: Loss 0.22821630949662058\n",
      "Epoch: 74\n",
      "Epoch 73: Loss 0.22821630949662058\n",
      "Epoch: 74\n",
      "Epoch 74: Loss 0.22818061951744764\n",
      "Epoch: 75\n",
      "Epoch 74: Loss 0.22818061951744764\n",
      "Epoch: 75\n",
      "Epoch 74: Loss 0.22818061951744764\n",
      "Epoch: 75\n",
      "Epoch 75: Loss 0.2281456785072063\n",
      "Epoch: 76\n",
      "Epoch 75: Loss 0.2281456785072063\n",
      "Epoch: 76\n",
      "Epoch 75: Loss 0.2281456785072063\n",
      "Epoch: 76\n",
      "Epoch 76: Loss 0.2281114575721276\n",
      "Epoch: 77\n",
      "Epoch 76: Loss 0.2281114575721276\n",
      "Epoch: 77\n",
      "Epoch 76: Loss 0.2281114575721276\n",
      "Epoch: 77\n",
      "Epoch 77: Loss 0.2280779293177613\n",
      "Epoch: 78\n",
      "Epoch 77: Loss 0.2280779293177613\n",
      "Epoch: 78\n",
      "Epoch 77: Loss 0.2280779293177613\n",
      "Epoch: 78\n",
      "Epoch 78: Loss 0.2280450677541128\n",
      "Epoch: 79\n",
      "Epoch 78: Loss 0.2280450677541128\n",
      "Epoch: 79\n",
      "Epoch 78: Loss 0.2280450677541128\n",
      "Epoch: 79\n",
      "Epoch 79: Loss 0.22801284819684664\n",
      "Epoch: 80\n",
      "Epoch 79: Loss 0.22801284819684664\n",
      "Epoch: 80\n",
      "Epoch 79: Loss 0.22801284819684664\n",
      "Epoch: 80\n",
      "Epoch 80: Loss 0.22798124716913865\n",
      "Epoch: 81\n",
      "Epoch 80: Loss 0.22798124716913865\n",
      "Epoch: 81\n",
      "Epoch 80: Loss 0.22798124716913865\n",
      "Epoch: 81\n",
      "Epoch 81: Loss 0.22795024230816693\n",
      "Epoch: 82\n",
      "Epoch 81: Loss 0.22795024230816693\n",
      "Epoch: 82\n",
      "Epoch 81: Loss 0.22795024230816693\n",
      "Epoch: 82\n",
      "Epoch 82: Loss 0.227919812279591\n",
      "Epoch: 83\n",
      "Epoch 82: Loss 0.227919812279591\n",
      "Epoch: 83\n",
      "Epoch 82: Loss 0.227919812279591\n",
      "Epoch: 83\n",
      "Epoch 83: Loss 0.22788993670278695\n",
      "Epoch: 84\n",
      "Epoch 83: Loss 0.22788993670278695\n",
      "Epoch: 84\n",
      "Epoch 83: Loss 0.22788993670278695\n",
      "Epoch: 84\n",
      "Epoch 84: Loss 0.2278605960886136\n",
      "Epoch: 85\n",
      "Epoch 84: Loss 0.2278605960886136\n",
      "Epoch: 85\n",
      "Epoch 84: Loss 0.2278605960886136\n",
      "Epoch: 85\n",
      "Epoch 85: Loss 0.22783177179063419\n",
      "Epoch: 86\n",
      "Epoch 85: Loss 0.22783177179063419\n",
      "Epoch: 86\n",
      "Epoch 85: Loss 0.22783177179063419\n",
      "Epoch: 86\n",
      "Epoch 86: Loss 0.2278034459695803\n",
      "Epoch: 87\n",
      "Epoch 86: Loss 0.2278034459695803\n",
      "Epoch: 87\n",
      "Epoch 86: Loss 0.2278034459695803\n",
      "Epoch: 87\n",
      "Epoch 87: Loss 0.2277756015703028\n",
      "Epoch: 88\n",
      "Epoch 87: Loss 0.2277756015703028\n",
      "Epoch: 88\n",
      "Epoch 87: Loss 0.2277756015703028\n",
      "Epoch: 88\n",
      "Epoch 88: Loss 0.2277482223089557\n",
      "Epoch: 89\n",
      "Epoch 88: Loss 0.2277482223089557\n",
      "Epoch: 89\n",
      "Epoch 88: Loss 0.2277482223089557\n",
      "Epoch: 89\n",
      "Epoch 89: Loss 0.22772129266803856\n",
      "Epoch: 90\n",
      "Epoch 89: Loss 0.22772129266803856\n",
      "Epoch: 90\n",
      "Epoch 89: Loss 0.22772129266803856\n",
      "Epoch: 90\n",
      "Epoch 90: Loss 0.22769479789605188\n",
      "Epoch: 91\n",
      "Epoch 90: Loss 0.22769479789605188\n",
      "Epoch: 91\n",
      "Epoch 90: Loss 0.22769479789605188\n",
      "Epoch: 91\n",
      "Epoch 91: Loss 0.2276687240084242\n",
      "Epoch: 92\n",
      "Epoch 91: Loss 0.2276687240084242\n",
      "Epoch: 92\n",
      "Epoch 91: Loss 0.2276687240084242\n",
      "Epoch: 92\n",
      "Epoch 92: Loss 0.22764305778645366\n",
      "Epoch: 93\n",
      "Epoch 92: Loss 0.22764305778645366\n",
      "Epoch: 93\n",
      "Epoch 92: Loss 0.22764305778645366\n",
      "Epoch: 93\n",
      "Epoch 93: Loss 0.2276177867712057\n",
      "Epoch: 94\n",
      "Epoch 93: Loss 0.2276177867712057\n",
      "Epoch: 94\n",
      "Epoch 93: Loss 0.2276177867712057\n",
      "Epoch: 94\n",
      "Epoch 94: Loss 0.22759289925010154\n",
      "Epoch: 95\n",
      "Epoch 94: Loss 0.22759289925010154\n",
      "Epoch: 95\n",
      "Epoch 94: Loss 0.22759289925010154\n",
      "Epoch: 95\n",
      "Epoch 95: Loss 0.22756838423447637\n",
      "Epoch: 96\n",
      "Epoch 95: Loss 0.22756838423447637\n",
      "Epoch: 96\n",
      "Epoch 95: Loss 0.22756838423447637\n",
      "Epoch: 96\n",
      "Epoch 96: Loss 0.22754423142756736\n",
      "Epoch: 97\n",
      "Epoch 96: Loss 0.22754423142756736\n",
      "Epoch: 97\n",
      "Epoch 96: Loss 0.22754423142756736\n",
      "Epoch: 97\n",
      "Epoch 97: Loss 0.22752043118301205\n",
      "Epoch: 98\n",
      "Epoch 97: Loss 0.22752043118301205\n",
      "Epoch: 98\n",
      "Epoch 97: Loss 0.22752043118301205\n",
      "Epoch: 98\n",
      "Epoch 98: Loss 0.2274969744550449\n",
      "Epoch: 99\n",
      "Epoch 98: Loss 0.2274969744550449\n",
      "Epoch: 99\n",
      "Epoch 98: Loss 0.2274969744550449\n",
      "Epoch: 99\n",
      "Epoch 99: Loss 0.2274738527420985\n",
      "Epoch 99: Loss 0.2274738527420985\n",
      "Epoch 99: Loss 0.2274738527420985\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 897 }, activation: Sigmoid }, LayerShape { layer_type: Dense { input_size: 897, output_size: 196 }, activation: ReLU }, LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 331 }, activation: Sigmoid }, LayerShape { layer_type: Dense { input_size: 331, output_size: 196 }, activation: ReLU }, LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 769 }, activation: Tanh }, LayerShape { layer_type: Dense { input_size: 769, output_size: 196 }, activation: ReLU }, LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Epoch 0: Loss 0.9277656889578538\n",
      "Epoch: 1\n",
      "Epoch 0: Loss 0.9279253184586514\n",
      "Epoch: 1\n",
      "Epoch 1: Loss 0.9274755596441182\n",
      "Epoch: 2\n",
      "Epoch 0: Loss 0.9282458147036207\n",
      "Epoch: 1\n",
      "Epoch 0: Loss 0.9285942320175452\n",
      "Epoch: 1\n",
      "Epoch 2: Loss 0.9274755596441182\n",
      "Epoch: 3\n",
      "Epoch 1: Loss 0.9274755596441182\n",
      "Epoch: 2\n",
      "Epoch 3: Loss 0.9274755596441182\n",
      "Epoch: 4\n",
      "Epoch 4: Loss 0.9274755596441182\n",
      "Epoch: 5\n",
      "Epoch 1: Loss 0.9274755596441182\n",
      "Epoch: 2\n",
      "Epoch 2: Loss 0.9274755596441182\n",
      "Epoch: 3\n",
      "Epoch 1: Loss 0.9274755596441182\n",
      "Epoch: 2\n",
      "Epoch 5: Loss 0.9274755596441182\n",
      "Epoch: 6\n",
      "Epoch 6: Loss 0.9274755596441182\n",
      "Epoch: 7\n",
      "Epoch 2: Loss 0.9274755596441182\n",
      "Epoch: 3\n",
      "Epoch 7: Loss 0.9274755596441182\n",
      "Epoch: 8\n",
      "Epoch 3: Loss 0.9274755596441182\n",
      "Epoch: 4\n",
      "Epoch 2: Loss 0.9274755596441182\n",
      "Epoch: 3\n",
      "Epoch 8: Loss 0.9274755596441182\n",
      "Epoch: 9\n",
      "Epoch 9: Loss 0.9274755596441182\n",
      "Epoch: 10\n",
      "Epoch 3: Loss 0.9274755596441182\n",
      "Epoch: 4\n",
      "Epoch 4: Loss 0.9274755596441182\n",
      "Epoch: 5\n",
      "Epoch 10: Loss 0.9274755596441182\n",
      "Epoch: 11\n",
      "Epoch 3: Loss 0.9274755596441182\n",
      "Epoch: 4\n",
      "Epoch 11: Loss 0.9274755596441182\n",
      "Epoch: 12\n",
      "Epoch 4: Loss 0.9274755596441182\n",
      "Epoch: 5\n",
      "Epoch 12: Loss 0.9274755596441182\n",
      "Epoch: 13\n",
      "Epoch 5: Loss 0.9274755596441182\n",
      "Epoch: 6\n",
      "Epoch 13: Loss 0.9274755596441182\n",
      "Epoch: 14\n",
      "Epoch 4: Loss 0.9274755596441182\n",
      "Epoch: 5\n",
      "Epoch 14: Loss 0.9274755596441182\n",
      "Epoch: 15\n",
      "Epoch 5: Loss 0.9274755596441182\n",
      "Epoch: 6\n",
      "Epoch 6: Loss 0.9274755596441182\n",
      "Epoch: 7\n",
      "Epoch 15: Loss 0.9274755596441182\n",
      "Epoch: 16\n",
      "Epoch 16: Loss 0.9274755596441182\n",
      "Epoch: 17\n",
      "Epoch 5: Loss 0.9274755596441182\n",
      "Epoch: 6\n",
      "Epoch 17: Loss 0.9274755596441182\n",
      "Epoch: 18\n",
      "Epoch 6: Loss 0.9274755596441182\n",
      "Epoch: 7\n",
      "Epoch 7: Loss 0.9274755596441182\n",
      "Epoch: 8\n",
      "Epoch 18: Loss 0.9274755596441182\n",
      "Epoch: 19\n",
      "Epoch 19: Loss 0.9274755596441182\n",
      "Epoch: 20\n",
      "Epoch 6: Loss 0.9274755596441182\n",
      "Epoch: 7\n",
      "Epoch 7: Loss 0.9274755596441182\n",
      "Epoch: 8\n",
      "Epoch 8: Loss 0.9274755596441182\n",
      "Epoch: 9\n",
      "Epoch 20: Loss 0.9274755596441182\n",
      "Epoch: 21\n",
      "Epoch 21: Loss 0.9274755596441182\n",
      "Epoch: 22\n",
      "Epoch 22: Loss 0.9274755596441182\n",
      "Epoch: 23\n",
      "Epoch 8: Loss 0.9274755596441182\n",
      "Epoch: 9\n",
      "Epoch 7: Loss 0.9274755596441182\n",
      "Epoch: 8\n",
      "Epoch 9: Loss 0.9274755596441182\n",
      "Epoch: 10\n",
      "Epoch 23: Loss 0.9274755596441182\n",
      "Epoch: 24\n",
      "Epoch 24: Loss 0.9274755596441182\n",
      "Epoch: 25\n",
      "Epoch 9: Loss 0.9274755596441182\n",
      "Epoch: 10\n",
      "Epoch 25: Loss 0.9274755596441182\n",
      "Epoch: 26\n",
      "Epoch 10: Loss 0.9274755596441182\n",
      "Epoch: 11\n",
      "Epoch 8: Loss 0.9274755596441182\n",
      "Epoch: 9\n",
      "Epoch 26: Loss 0.9274755596441182\n",
      "Epoch: 27\n",
      "Epoch 27: Loss 0.9274755596441182\n",
      "Epoch: 28\n",
      "Epoch 10: Loss 0.9274755596441182\n",
      "Epoch: 11\n",
      "Epoch 11: Loss 0.9274755596441182\n",
      "Epoch: 12\n",
      "Epoch 28: Loss 0.9274755596441182\n",
      "Epoch: 29\n",
      "Epoch 9: Loss 0.9274755596441182\n",
      "Epoch: 10\n",
      "Epoch 29: Loss 0.9274755596441182\n",
      "Epoch: 30\n",
      "Epoch 11: Loss 0.9274755596441182\n",
      "Epoch: 12\n",
      "Epoch 30: Loss 0.9274755596441182\n",
      "Epoch: 31\n",
      "Epoch 12: Loss 0.9274755596441182\n",
      "Epoch: 13\n",
      "Epoch 10: Loss 0.9274755596441182\n",
      "Epoch: 11\n",
      "Epoch 31: Loss 0.9274755596441182\n",
      "Epoch: 32\n",
      "Epoch 32: Loss 0.9274755596441182\n",
      "Epoch: 33\n",
      "Epoch 12: Loss 0.9274755596441182\n",
      "Epoch: 13\n",
      "Epoch 13: Loss 0.9274755596441182\n",
      "Epoch: 14\n",
      "Epoch 33: Loss 0.9274755596441182\n",
      "Epoch: 34\n",
      "Epoch 11: Loss 0.9274755596441182\n",
      "Epoch: 12\n",
      "Epoch 34: Loss 0.9274755596441182\n",
      "Epoch: 35\n",
      "Epoch 13: Loss 0.9274755596441182\n",
      "Epoch: 14\n",
      "Epoch 14: Loss 0.9274755596441182\n",
      "Epoch: 15\n",
      "Epoch 35: Loss 0.9274755596441182\n",
      "Epoch: 36\n",
      "Epoch 36: Loss 0.9274755596441182\n",
      "Epoch: 37\n",
      "Epoch 12: Loss 0.9274755596441182\n",
      "Epoch: 13\n",
      "Epoch 37: Loss 0.9274755596441182\n",
      "Epoch: 38\n",
      "Epoch 15: Loss 0.9274755596441182\n",
      "Epoch: 16\n",
      "Epoch 14: Loss 0.9274755596441182\n",
      "Epoch: 15\n",
      "Epoch 38: Loss 0.9274755596441182\n",
      "Epoch: 39\n",
      "Epoch 39: Loss 0.9274755596441182\n",
      "Epoch: 40\n",
      "Epoch 13: Loss 0.9274755596441182\n",
      "Epoch: 14\n",
      "Epoch 16: Loss 0.9274755596441182\n",
      "Epoch: 17\n",
      "Epoch 15: Loss 0.9274755596441182\n",
      "Epoch: 16\n",
      "Epoch 40: Loss 0.9274755596441182\n",
      "Epoch: 41\n",
      "Epoch 41: Loss 0.9274755596441182\n",
      "Epoch: 42\n",
      "Epoch 42: Loss 0.9274755596441182\n",
      "Epoch: 43\n",
      "Epoch 17: Loss 0.9274755596441182\n",
      "Epoch: 18\n",
      "Epoch 16: Loss 0.9274755596441182\n",
      "Epoch: 17\n",
      "Epoch 14: Loss 0.9274755596441182\n",
      "Epoch: 15\n",
      "Epoch 43: Loss 0.9274755596441182\n",
      "Epoch: 44\n",
      "Epoch 44: Loss 0.9274755596441182\n",
      "Epoch: 45\n",
      "Epoch 18: Loss 0.9274755596441182\n",
      "Epoch: 19\n",
      "Epoch 17: Loss 0.9274755596441182\n",
      "Epoch: 18\n",
      "Epoch 45: Loss 0.9274755596441182\n",
      "Epoch: 46\n",
      "Epoch 15: Loss 0.9274755596441182\n",
      "Epoch: 16\n",
      "Epoch 46: Loss 0.9274755596441182\n",
      "Epoch: 47\n",
      "Epoch 19: Loss 0.9274755596441182\n",
      "Epoch: 20\n",
      "Epoch 47: Loss 0.9274755596441182\n",
      "Epoch: 48\n",
      "Epoch 18: Loss 0.9274755596441182\n",
      "Epoch: 19\n",
      "Epoch 48: Loss 0.9274755596441182\n",
      "Epoch: 49\n",
      "Epoch 16: Loss 0.9274755596441182\n",
      "Epoch: 17\n",
      "Epoch 49: Loss 0.9274755596441182\n",
      "Epoch: 50\n",
      "Epoch 20: Loss 0.9274755596441182\n",
      "Epoch: 21\n",
      "Epoch 19: Loss 0.9274755596441182\n",
      "Epoch: 20\n",
      "Epoch 50: Loss 0.9274755596441182\n",
      "Epoch: 51\n",
      "Epoch 51: Loss 0.9274755596441182\n",
      "Epoch: 52\n",
      "Epoch 17: Loss 0.9274755596441182\n",
      "Epoch: 18\n",
      "Epoch 21: Loss 0.9274755596441182\n",
      "Epoch: 22\n",
      "Epoch 52: Loss 0.9274755596441182\n",
      "Epoch: 53\n",
      "Epoch 20: Loss 0.9274755596441182\n",
      "Epoch: 21\n",
      "Epoch 53: Loss 0.9274755596441182\n",
      "Epoch: 54\n",
      "Epoch 54: Loss 0.9274755596441182\n",
      "Epoch: 55\n",
      "Epoch 18: Loss 0.9274755596441182\n",
      "Epoch: 19\n",
      "Epoch 22: Loss 0.9274755596441182\n",
      "Epoch: 23\n",
      "Epoch 21: Loss 0.9274755596441182\n",
      "Epoch: 22\n",
      "Epoch 55: Loss 0.9274755596441182\n",
      "Epoch: 56\n",
      "Epoch 56: Loss 0.9274755596441182\n",
      "Epoch: 57\n",
      "Epoch 23: Loss 0.9274755596441182\n",
      "Epoch: 24\n",
      "Epoch 19: Loss 0.9274755596441182\n",
      "Epoch: 20\n",
      "Epoch 57: Loss 0.9274755596441182\n",
      "Epoch: 58\n",
      "Epoch 22: Loss 0.9274755596441182\n",
      "Epoch: 23\n",
      "Epoch 58: Loss 0.9274755596441182\n",
      "Epoch: 59\n",
      "Epoch 59: Loss 0.9274755596441182\n",
      "Epoch: 60\n",
      "Epoch 24: Loss 0.9274755596441182\n",
      "Epoch: 25\n",
      "Epoch 20: Loss 0.9274755596441182\n",
      "Epoch: 21\n",
      "Epoch 23: Loss 0.9274755596441182\n",
      "Epoch: 24\n",
      "Epoch 60: Loss 0.9274755596441182\n",
      "Epoch: 61\n",
      "Epoch 61: Loss 0.9274755596441182\n",
      "Epoch: 62\n",
      "Epoch 25: Loss 0.9274755596441182\n",
      "Epoch: 26\n",
      "Epoch 62: Loss 0.9274755596441182\n",
      "Epoch: 63\n",
      "Epoch 24: Loss 0.9274755596441182\n",
      "Epoch: 25\n",
      "Epoch 21: Loss 0.9274755596441182\n",
      "Epoch: 22\n",
      "Epoch 63: Loss 0.9274755596441182\n",
      "Epoch: 64\n",
      "Epoch 64: Loss 0.9274755596441182\n",
      "Epoch: 65\n",
      "Epoch 26: Loss 0.9274755596441182\n",
      "Epoch: 27\n",
      "Epoch 65: Loss 0.9274755596441182\n",
      "Epoch: 66\n",
      "Epoch 25: Loss 0.9274755596441182\n",
      "Epoch: 26\n",
      "Epoch 22: Loss 0.9274755596441182\n",
      "Epoch: 23\n",
      "Epoch 66: Loss 0.9274755596441182\n",
      "Epoch: 67\n",
      "Epoch 27: Loss 0.9274755596441182\n",
      "Epoch: 28\n",
      "Epoch 67: Loss 0.9274755596441182\n",
      "Epoch: 68\n",
      "Epoch 26: Loss 0.9274755596441182\n",
      "Epoch: 27\n",
      "Epoch 68: Loss 0.9274755596441182\n",
      "Epoch: 69\n",
      "Epoch 23: Loss 0.9274755596441182\n",
      "Epoch: 24\n",
      "Epoch 28: Loss 0.9274755596441182\n",
      "Epoch: 29\n",
      "Epoch 69: Loss 0.9274755596441182\n",
      "Epoch: 70\n",
      "Epoch 70: Loss 0.9274755596441182\n",
      "Epoch: 71\n",
      "Epoch 27: Loss 0.9274755596441182\n",
      "Epoch: 28\n",
      "Epoch 71: Loss 0.9274755596441182\n",
      "Epoch: 72\n",
      "Epoch 29: Loss 0.9274755596441182\n",
      "Epoch: 30\n",
      "Epoch 24: Loss 0.9274755596441182\n",
      "Epoch: 25\n",
      "Epoch 72: Loss 0.9274755596441182\n",
      "Epoch: 73\n",
      "Epoch 28: Loss 0.9274755596441182\n",
      "Epoch: 29\n",
      "Epoch 73: Loss 0.9274755596441182\n",
      "Epoch: 74\n",
      "Epoch 30: Loss 0.9274755596441182\n",
      "Epoch: 31\n",
      "Epoch 74: Loss 0.9274755596441182\n",
      "Epoch: 75\n",
      "Epoch 25: Loss 0.9274755596441182\n",
      "Epoch: 26\n",
      "Epoch 75: Loss 0.9274755596441182\n",
      "Epoch: 76\n",
      "Epoch 29: Loss 0.9274755596441182\n",
      "Epoch: 30\n",
      "Epoch 76: Loss 0.9274755596441182\n",
      "Epoch: 77\n",
      "Epoch 31: Loss 0.9274755596441182\n",
      "Epoch: 32\n",
      "Epoch 77: Loss 0.9274755596441182\n",
      "Epoch: 78\n",
      "Epoch 26: Loss 0.9274755596441182\n",
      "Epoch: 27\n",
      "Epoch 30: Loss 0.9274755596441182\n",
      "Epoch: 31\n",
      "Epoch 78: Loss 0.9274755596441182\n",
      "Epoch: 79\n",
      "Epoch 32: Loss 0.9274755596441182\n",
      "Epoch: 33\n",
      "Epoch 79: Loss 0.9274755596441182\n",
      "Epoch: 80\n",
      "Epoch 80: Loss 0.9274755596441182\n",
      "Epoch: 81\n",
      "Epoch 27: Loss 0.9274755596441182\n",
      "Epoch: 28\n",
      "Epoch 31: Loss 0.9274755596441182\n",
      "Epoch: 32\n",
      "Epoch 81: Loss 0.9274755596441182\n",
      "Epoch: 82\n",
      "Epoch 33: Loss 0.9274755596441182\n",
      "Epoch: 34\n",
      "Epoch 82: Loss 0.9274755596441182\n",
      "Epoch: 83\n",
      "Epoch 32: Loss 0.9274755596441182\n",
      "Epoch: 33\n",
      "Epoch 28: Loss 0.9274755596441182\n",
      "Epoch: 29\n",
      "Epoch 83: Loss 0.9274755596441182\n",
      "Epoch: 84\n",
      "Epoch 34: Loss 0.9274755596441182\n",
      "Epoch: 35\n",
      "Epoch 84: Loss 0.9274755596441182\n",
      "Epoch: 85\n",
      "Epoch 85: Loss 0.9274755596441182\n",
      "Epoch: 86\n",
      "Epoch 33: Loss 0.9274755596441182\n",
      "Epoch: 34\n",
      "Epoch 29: Loss 0.9274755596441182\n",
      "Epoch: 30\n",
      "Epoch 35: Loss 0.9274755596441182\n",
      "Epoch: 36\n",
      "Epoch 86: Loss 0.9274755596441182\n",
      "Epoch: 87\n",
      "Epoch 87: Loss 0.9274755596441182\n",
      "Epoch: 88\n",
      "Epoch 34: Loss 0.9274755596441182\n",
      "Epoch: 35\n",
      "Epoch 88: Loss 0.9274755596441182\n",
      "Epoch: 89\n",
      "Epoch 36: Loss 0.9274755596441182\n",
      "Epoch: 37\n",
      "Epoch 30: Loss 0.9274755596441182\n",
      "Epoch: 31\n",
      "Epoch 89: Loss 0.9274755596441182\n",
      "Epoch: 90\n",
      "Epoch 90: Loss 0.9274755596441182\n",
      "Epoch: 91\n",
      "Epoch 35: Loss 0.9274755596441182\n",
      "Epoch: 36\n",
      "Epoch 37: Loss 0.9274755596441182\n",
      "Epoch: 38\n",
      "Epoch 91: Loss 0.9274755596441182\n",
      "Epoch: 92\n",
      "Epoch 31: Loss 0.9274755596441182\n",
      "Epoch: 32\n",
      "Epoch 92: Loss 0.9274755596441182\n",
      "Epoch: 93\n",
      "Epoch 36: Loss 0.9274755596441182\n",
      "Epoch: 37\n",
      "Epoch 93: Loss 0.9274755596441182\n",
      "Epoch: 94\n",
      "Epoch 38: Loss 0.9274755596441182\n",
      "Epoch: 39\n",
      "Epoch 94: Loss 0.9274755596441182\n",
      "Epoch: 95\n",
      "Epoch 32: Loss 0.9274755596441182\n",
      "Epoch: 33\n",
      "Epoch 95: Loss 0.9274755596441182\n",
      "Epoch: 96\n",
      "Epoch 37: Loss 0.9274755596441182\n",
      "Epoch: 38\n",
      "Epoch 39: Loss 0.9274755596441182\n",
      "Epoch: 40\n",
      "Epoch 96: Loss 0.9274755596441182\n",
      "Epoch: 97\n",
      "Epoch 97: Loss 0.9274755596441182\n",
      "Epoch: 98\n",
      "Epoch 33: Loss 0.9274755596441182\n",
      "Epoch: 34\n",
      "Epoch 38: Loss 0.9274755596441182\n",
      "Epoch: 39\n",
      "Epoch 98: Loss 0.9274755596441182\n",
      "Epoch: 99\n",
      "Epoch 40: Loss 0.9274755596441182\n",
      "Epoch: 41\n",
      "Epoch 99: Loss 0.9274755596441182\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Epoch 0: Loss 0.32277414660446563\n",
      "Epoch: 1\n",
      "Epoch 1: Loss 0.26502724016561197\n",
      "Epoch: 2\n",
      "Epoch 2: Loss 0.25539231975668025\n",
      "Epoch: 3\n",
      "Epoch 3: Loss 0.2502114973723556\n",
      "Epoch: 4\n",
      "Epoch 4: Loss 0.24681649989584786\n",
      "Epoch: 5\n",
      "Epoch 5: Loss 0.24437722125477435\n",
      "Epoch: 6\n",
      "Epoch 6: Loss 0.24250877002618654\n",
      "Epoch: 7\n",
      "Epoch 7: Loss 0.24102103770605712\n",
      "Epoch: 8\n",
      "Epoch 8: Loss 0.23980837425952023\n",
      "Epoch: 9\n",
      "Epoch 9: Loss 0.23880081195432196\n",
      "Epoch: 10\n",
      "Epoch 10: Loss 0.23794887314106106\n",
      "Epoch: 11\n",
      "Epoch 11: Loss 0.2372165979629189\n",
      "Epoch: 12\n",
      "Epoch 12: Loss 0.2365774873838539\n",
      "Epoch: 13\n",
      "Epoch 13: Loss 0.2360119464004573\n",
      "Epoch: 14\n",
      "Epoch 14: Loss 0.23550630062490882\n",
      "Epoch: 15\n",
      "Epoch 15: Loss 0.23505127743678486\n",
      "Epoch: 16\n",
      "Epoch 16: Loss 0.2346399409357362\n",
      "Epoch: 17\n",
      "Epoch 17: Loss 0.23426634587791317\n",
      "Epoch: 18\n",
      "Epoch 18: Loss 0.23392516915498182\n",
      "Epoch: 19\n",
      "Epoch 19: Loss 0.23361180004190338\n",
      "Epoch: 20\n",
      "Epoch 20: Loss 0.23332241542927085\n",
      "Epoch: 21\n",
      "Epoch 21: Loss 0.23305392559360416\n",
      "Epoch: 22\n",
      "Epoch 22: Loss 0.23280384978276558\n",
      "Epoch: 23\n",
      "Epoch 23: Loss 0.232570183046648\n",
      "Epoch: 24\n",
      "Epoch 24: Loss 0.23235128194902288\n",
      "Epoch: 25\n",
      "Epoch 25: Loss 0.23214577623746074\n",
      "Epoch: 26\n",
      "Epoch 26: Loss 0.2319525038040332\n",
      "Epoch: 27\n",
      "Epoch 27: Loss 0.23177046240178667\n",
      "Epoch: 28\n",
      "Epoch 28: Loss 0.2315987725179126\n",
      "Epoch: 29\n",
      "Epoch 29: Loss 0.23143664848110868\n",
      "Epoch: 30\n",
      "Epoch 30: Loss 0.23128337649983138\n",
      "Epoch: 31\n",
      "Epoch 31: Loss 0.2311382987120927\n",
      "Epoch: 32\n",
      "Epoch 32: Loss 0.23100080236562684\n",
      "Epoch: 33\n",
      "Epoch 33: Loss 0.2308703133103028\n",
      "Epoch: 34\n",
      "Epoch 34: Loss 0.2307462929759982\n",
      "Epoch: 35\n",
      "Epoch 35: Loss 0.23062823786661713\n",
      "Epoch: 36\n",
      "Epoch 36: Loss 0.23051568043403395\n",
      "Epoch: 37\n",
      "Epoch 37: Loss 0.23040819015708056\n",
      "Epoch: 38\n",
      "Epoch 38: Loss 0.23030537384603184\n",
      "Epoch: 39\n",
      "Epoch 39: Loss 0.23020687462989714\n",
      "Epoch: 40\n",
      "Epoch 40: Loss 0.23011236965856513\n",
      "Epoch: 41\n",
      "Epoch 41: Loss 0.23002156704824023\n",
      "Epoch: 42\n",
      "Epoch 42: Loss 0.22993420278891388\n",
      "Epoch: 43\n",
      "Epoch 43: Loss 0.22985003814503607\n",
      "Epoch: 44\n",
      "Epoch 44: Loss 0.22976885767656183\n",
      "Epoch: 45\n",
      "Epoch 45: Loss 0.22969046766290863\n",
      "Epoch: 46\n",
      "Epoch 46: Loss 0.22961469460255834\n",
      "Epoch: 47\n",
      "Epoch 47: Loss 0.2295413835647168\n",
      "Epoch: 48\n",
      "Epoch 48: Loss 0.22947039634538766\n",
      "Epoch: 49\n",
      "Epoch 49: Loss 0.22940160950983216\n",
      "Epoch: 50\n",
      "Epoch 50: Loss 0.22933491244697937\n",
      "Epoch: 51\n",
      "Epoch 51: Loss 0.22927020554183214\n",
      "Epoch: 52\n",
      "Epoch 52: Loss 0.22920739852771613\n",
      "Epoch: 53\n",
      "Epoch 53: Loss 0.22914640903845843\n",
      "Epoch: 54\n",
      "Epoch 54: Loss 0.22908716135324328\n",
      "Epoch: 55\n",
      "Epoch 55: Loss 0.2290295853138948\n",
      "Epoch: 56\n",
      "Epoch 56: Loss 0.22897361539242939\n",
      "Epoch: 57\n",
      "Epoch 57: Loss 0.2289191898902632\n",
      "Epoch: 58\n",
      "Epoch 58: Loss 0.22886625025658222\n",
      "Epoch: 59\n",
      "Epoch 59: Loss 0.22881474051905262\n",
      "Epoch: 60\n",
      "Epoch 60: Loss 0.22876460682374364\n",
      "Epoch: 61\n",
      "Epoch 61: Loss 0.22871579708223141\n",
      "Epoch: 62\n",
      "Epoch 62: Loss 0.22866826072224636\n",
      "Epoch: 63\n",
      "Epoch 63: Loss 0.22862194853464754\n",
      "Epoch: 64\n",
      "Epoch 64: Loss 0.2285768126047131\n",
      "Epoch: 65\n",
      "Epoch 65: Loss 0.22853280631182252\n",
      "Epoch: 66\n",
      "Epoch 66: Loss 0.22848988437876902\n",
      "Epoch: 67\n",
      "Epoch 67: Loss 0.2284480029511711\n",
      "Epoch: 68\n",
      "Epoch 68: Loss 0.22840711968885288\n",
      "Epoch: 69\n",
      "Epoch 69: Loss 0.22836719385371282\n",
      "Epoch: 70\n",
      "Epoch 70: Loss 0.22832818638244134\n",
      "Epoch: 71\n",
      "Epoch 39: Loss 0.9274755596441182\n",
      "Epoch: 40\n",
      "Epoch 71: Loss 0.22829005993626317\n",
      "Epoch: 72\n",
      "Epoch 72: Loss 0.22825277892388332\n",
      "Epoch: 73\n",
      "Epoch 73: Loss 0.22821630949662058\n",
      "Epoch: 74\n",
      "Epoch 74: Loss 0.22818061951744764\n",
      "Epoch: 75\n",
      "Epoch 75: Loss 0.2281456785072063\n",
      "Epoch: 76\n",
      "Epoch 76: Loss 0.2281114575721276\n",
      "Epoch: 77\n",
      "Epoch 77: Loss 0.2280779293177613\n",
      "Epoch: 78\n",
      "Epoch 78: Loss 0.2280450677541128\n",
      "Epoch: 79\n",
      "Epoch 79: Loss 0.22801284819684664\n",
      "Epoch: 80\n",
      "Epoch 80: Loss 0.22798124716913865\n",
      "Epoch: 81\n",
      "Epoch 81: Loss 0.22795024230816693\n",
      "Epoch: 82\n",
      "Epoch 82: Loss 0.227919812279591\n",
      "Epoch: 83\n",
      "Epoch 83: Loss 0.22788993670278695\n",
      "Epoch: 84\n",
      "Epoch 84: Loss 0.2278605960886136\n",
      "Epoch: 85\n",
      "Epoch 34: Loss 0.9274755596441182\n",
      "Epoch: 35\n",
      "Epoch 85: Loss 0.22783177179063419\n",
      "Epoch: 86\n",
      "Epoch 86: Loss 0.2278034459695803\n",
      "Epoch: 87\n",
      "Epoch 87: Loss 0.2277756015703028\n",
      "Epoch: 88\n",
      "Epoch 88: Loss 0.2277482223089557\n",
      "Epoch: 89\n",
      "Epoch 89: Loss 0.22772129266803856\n",
      "Epoch: 90\n",
      "Epoch 90: Loss 0.22769479789605188\n",
      "Epoch: 91\n",
      "Epoch 91: Loss 0.2276687240084242\n",
      "Epoch: 92\n",
      "Epoch 92: Loss 0.22764305778645366\n",
      "Epoch: 93\n",
      "Epoch 93: Loss 0.2276177867712057\n",
      "Epoch: 94\n",
      "Epoch 41: Loss 0.9274755596441182\n",
      "Epoch: 42\n",
      "Epoch 94: Loss 0.22759289925010154\n",
      "Epoch: 95\n",
      "Epoch 95: Loss 0.22756838423447637\n",
      "Epoch: 96\n",
      "Epoch 96: Loss 0.22754423142756736\n",
      "Epoch: 97\n",
      "Epoch 97: Loss 0.22752043118301205\n",
      "Epoch: 98\n",
      "Epoch 98: Loss 0.2274969744550449\n",
      "Epoch: 99\n",
      "Epoch 99: Loss 0.2274738527420985\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: ReLU }] }\n",
      "Epoch: 0\n",
      "Epoch 0: Loss 1.000027032866724\n",
      "Epoch: 1\n",
      "Epoch 1: Loss 1\n",
      "Epoch: 2\n",
      "Epoch 2: Loss 1\n",
      "Epoch: 3\n",
      "Epoch 3: Loss 1\n",
      "Epoch: 4\n",
      "Epoch 4: Loss 1\n",
      "Epoch: 5\n",
      "Epoch 5: Loss 1\n",
      "Epoch: 6\n",
      "Epoch 6: Loss 1\n",
      "Epoch: 7\n",
      "Epoch 7: Loss 1\n",
      "Epoch: 8\n",
      "Epoch 8: Loss 1\n",
      "Epoch: 9\n",
      "Epoch 9: Loss 1\n",
      "Epoch: 10\n",
      "Epoch 10: Loss 1\n",
      "Epoch: 11\n",
      "Epoch 11: Loss 1\n",
      "Epoch: 12\n",
      "Epoch 12: Loss 1\n",
      "Epoch: 13\n",
      "Epoch 13: Loss 1\n",
      "Epoch: 14\n",
      "Epoch 14: Loss 1\n",
      "Epoch: 15\n",
      "Epoch 15: Loss 1\n",
      "Epoch: 16\n",
      "Epoch 16: Loss 1\n",
      "Epoch: 17\n",
      "Epoch 17: Loss 1\n",
      "Epoch: 18\n",
      "Epoch 18: Loss 1\n",
      "Epoch: 19\n",
      "Epoch 19: Loss 1\n",
      "Epoch: 20\n",
      "Epoch 20: Loss 1\n",
      "Epoch: 21\n",
      "Epoch 21: Loss 1\n",
      "Epoch: 22\n",
      "Epoch 22: Loss 1\n",
      "Epoch: 23\n",
      "Epoch 23: Loss 1\n",
      "Epoch: 24\n",
      "Epoch 24: Loss 1\n",
      "Epoch: 25\n",
      "Epoch 25: Loss 1\n",
      "Epoch: 26\n",
      "Epoch 26: Loss 1\n",
      "Epoch: 27\n",
      "Epoch 27: Loss 1\n",
      "Epoch: 28\n",
      "Epoch 28: Loss 1\n",
      "Epoch: 29\n",
      "Epoch 29: Loss 1\n",
      "Epoch: 30\n",
      "Epoch 30: Loss 1\n",
      "Epoch: 31\n",
      "Epoch 31: Loss 1\n",
      "Epoch: 32\n",
      "Epoch 32: Loss 1\n",
      "Epoch: 33\n",
      "Epoch 33: Loss 1\n",
      "Epoch: 34\n",
      "Epoch 34: Loss 1\n",
      "Epoch: 35\n",
      "Epoch 35: Loss 1\n",
      "Epoch: 36\n",
      "Epoch 36: Loss 1\n",
      "Epoch: 37\n",
      "Epoch 37: Loss 1\n",
      "Epoch: 38\n",
      "Epoch 38: Loss 1\n",
      "Epoch: 39\n",
      "Epoch 39: Loss 1\n",
      "Epoch: 40\n",
      "Epoch 40: Loss 1\n",
      "Epoch: 41\n",
      "Epoch 41: Loss 1\n",
      "Epoch: 42\n",
      "Epoch 42: Loss 1\n",
      "Epoch: 43\n",
      "Epoch 43: Loss 1\n",
      "Epoch: 44\n",
      "Epoch 44: Loss 1\n",
      "Epoch: 45\n",
      "Epoch 45: Loss 1\n",
      "Epoch: 46\n",
      "Epoch 46: Loss 1\n",
      "Epoch: 47\n",
      "Epoch 47: Loss 1\n",
      "Epoch: 48\n",
      "Epoch 48: Loss 1\n",
      "Epoch: 49\n",
      "Epoch 49: Loss 1\n",
      "Epoch: 50\n",
      "Epoch 50: Loss 1\n",
      "Epoch: 51\n",
      "Epoch 51: Loss 1\n",
      "Epoch: 52\n",
      "Epoch 52: Loss 1\n",
      "Epoch: 53\n",
      "Epoch 53: Loss 1\n",
      "Epoch: 54\n",
      "Epoch 54: Loss 1\n",
      "Epoch: 55\n",
      "Epoch 55: Loss 1\n",
      "Epoch: 56\n",
      "Epoch 56: Loss 1\n",
      "Epoch: 57\n",
      "Epoch 57: Loss 1\n",
      "Epoch: 58\n",
      "Epoch 58: Loss 1\n",
      "Epoch: 59\n",
      "Epoch 59: Loss 1\n",
      "Epoch: 60\n",
      "Epoch 60: Loss 1\n",
      "Epoch: 61\n",
      "Epoch 61: Loss 1\n",
      "Epoch: 62\n",
      "Epoch 62: Loss 1\n",
      "Epoch: 63\n",
      "Epoch 63: Loss 1\n",
      "Epoch: 64\n",
      "Epoch 64: Loss 1\n",
      "Epoch: 65\n",
      "Epoch 65: Loss 1\n",
      "Epoch: 66\n",
      "Epoch 66: Loss 1\n",
      "Epoch: 67\n",
      "Epoch 67: Loss 1\n",
      "Epoch: 68\n",
      "Epoch 68: Loss 1\n",
      "Epoch: 69\n",
      "Epoch 69: Loss 1\n",
      "Epoch: 70\n",
      "Epoch 70: Loss 1\n",
      "Epoch: 71\n",
      "Epoch 71: Loss 1\n",
      "Epoch: 72\n",
      "Epoch 72: Loss 1\n",
      "Epoch: 73\n",
      "Epoch 73: Loss 1\n",
      "Epoch: 74\n",
      "Epoch 74: Loss 1\n",
      "Epoch: 75\n",
      "Epoch 75: Loss 1\n",
      "Epoch: 76\n",
      "Epoch 76: Loss 1\n",
      "Epoch: 77\n",
      "Epoch 77: Loss 1\n",
      "Epoch: 78\n",
      "Epoch 78: Loss 1\n",
      "Epoch: 79\n",
      "Epoch 79: Loss 1\n",
      "Epoch: 80\n",
      "Epoch 80: Loss 1\n",
      "Epoch: 81\n",
      "Epoch 81: Loss 1\n",
      "Epoch: 82\n",
      "Epoch 82: Loss 1\n",
      "Epoch: 83\n",
      "Epoch 83: Loss 1\n",
      "Epoch: 84\n",
      "Epoch 84: Loss 1\n",
      "Epoch: 85\n",
      "Epoch 85: Loss 1\n",
      "Epoch: 86\n",
      "Epoch 86: Loss 1\n",
      "Epoch: 87\n",
      "Epoch 87: Loss 1\n",
      "Epoch: 88\n",
      "Epoch 88: Loss 1\n",
      "Epoch: 89\n",
      "Epoch 89: Loss 1\n",
      "Epoch: 90\n",
      "Epoch 90: Loss 1\n",
      "Epoch: 91\n",
      "Epoch 91: Loss 1\n",
      "Epoch: 92\n",
      "Epoch 92: Loss 1\n",
      "Epoch: 93\n",
      "Epoch 93: Loss 1\n",
      "Epoch: 94\n",
      "Epoch 94: Loss 1\n",
      "Epoch: 95\n",
      "Epoch 95: Loss 1\n",
      "Epoch: 96\n",
      "Epoch 96: Loss 1\n",
      "Epoch: 97\n",
      "Epoch 97: Loss 1\n",
      "Epoch: 98\n",
      "Epoch 98: Loss 1\n",
      "Epoch: 99\n",
      "Epoch 99: Loss 1\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: ReLU }] }\n",
      "Epoch: 0\n",
      "Epoch 0: Loss 1.000027032866724\n",
      "Epoch: 1\n",
      "Epoch 1: Loss 1\n",
      "Epoch: 2\n",
      "Epoch 2: Loss 1\n",
      "Epoch: 3\n",
      "Epoch 3: Loss 1\n",
      "Epoch: 4\n",
      "Epoch 4: Loss 1\n",
      "Epoch: 5\n",
      "Epoch 5: Loss 1\n",
      "Epoch: 6\n",
      "Epoch 6: Loss 1\n",
      "Epoch: 7\n",
      "Epoch 7: Loss 1\n",
      "Epoch: 8\n",
      "Epoch 8: Loss 1\n",
      "Epoch: 9\n",
      "Epoch 9: Loss 1\n",
      "Epoch: 10\n",
      "Epoch 10: Loss 1\n",
      "Epoch: 11\n",
      "Epoch 11: Loss 1\n",
      "Epoch: 12\n",
      "Epoch 12: Loss 1\n",
      "Epoch: 13\n",
      "Epoch 13: Loss 1\n",
      "Epoch: 14\n",
      "Epoch 14: Loss 1\n",
      "Epoch: 15\n",
      "Epoch 15: Loss 1\n",
      "Epoch: 16\n",
      "Epoch 16: Loss 1\n",
      "Epoch: 17\n",
      "Epoch 17: Loss 1\n",
      "Epoch: 18\n",
      "Epoch 18: Loss 1\n",
      "Epoch: 19\n",
      "Epoch 19: Loss 1\n",
      "Epoch: 20\n",
      "Epoch 20: Loss 1\n",
      "Epoch: 21\n",
      "Epoch 21: Loss 1\n",
      "Epoch: 22\n",
      "Epoch 22: Loss 1\n",
      "Epoch: 23\n",
      "Epoch 23: Loss 1\n",
      "Epoch: 24\n",
      "Epoch 24: Loss 1\n",
      "Epoch: 25\n",
      "Epoch 25: Loss 1\n",
      "Epoch: 26\n",
      "Epoch 26: Loss 1\n",
      "Epoch: 27\n",
      "Epoch 27: Loss 1\n",
      "Epoch: 28\n",
      "Epoch 28: Loss 1\n",
      "Epoch: 29\n",
      "Epoch 29: Loss 1\n",
      "Epoch: 30\n",
      "Epoch 30: Loss 1\n",
      "Epoch: 31\n",
      "Epoch 31: Loss 1\n",
      "Epoch: 32\n",
      "Epoch 32: Loss 1\n",
      "Epoch: 33\n",
      "Epoch 33: Loss 1\n",
      "Epoch: 34\n",
      "Epoch 34: Loss 1\n",
      "Epoch: 35\n",
      "Epoch 35: Loss 1\n",
      "Epoch: 36\n",
      "Epoch 36: Loss 1\n",
      "Epoch: 37\n",
      "Epoch 37: Loss 1\n",
      "Epoch: 38\n",
      "Epoch 38: Loss 1\n",
      "Epoch: 39\n",
      "Epoch 39: Loss 1\n",
      "Epoch: 40\n",
      "Epoch 40: Loss 1\n",
      "Epoch: 41\n",
      "Epoch 41: Loss 1\n",
      "Epoch: 42\n",
      "Epoch 42: Loss 1\n",
      "Epoch: 43\n",
      "Epoch 43: Loss 1\n",
      "Epoch: 44\n",
      "Epoch 44: Loss 1\n",
      "Epoch: 45\n",
      "Epoch 45: Loss 1\n",
      "Epoch: 46\n",
      "Epoch 46: Loss 1\n",
      "Epoch: 47\n",
      "Epoch 47: Loss 1\n",
      "Epoch: 48\n",
      "Epoch 48: Loss 1\n",
      "Epoch: 49\n",
      "Epoch 49: Loss 1\n",
      "Epoch: 50\n",
      "Epoch 50: Loss 1\n",
      "Epoch: 51\n",
      "Epoch 51: Loss 1\n",
      "Epoch: 52\n",
      "Epoch 40: Loss 0.9274755596441182\n",
      "Epoch: 41\n",
      "Epoch 52: Loss 1\n",
      "Epoch: 53\n",
      "Epoch 53: Loss 1\n",
      "Epoch: 54\n",
      "Epoch 54: Loss 1\n",
      "Epoch: 55\n",
      "Epoch 55: Loss 1\n",
      "Epoch: 56\n",
      "Epoch 56: Loss 1\n",
      "Epoch: 57\n",
      "Epoch 57: Loss 1\n",
      "Epoch: 58\n",
      "Epoch 58: Loss 1\n",
      "Epoch: 59\n",
      "Epoch 59: Loss 1\n",
      "Epoch: 60\n",
      "Epoch 60: Loss 1\n",
      "Epoch: 61\n",
      "Epoch 61: Loss 1\n",
      "Epoch: 62\n",
      "Epoch 62: Loss 1\n",
      "Epoch: 63\n",
      "Epoch 63: Loss 1\n",
      "Epoch: 64\n",
      "Epoch 64: Loss 1\n",
      "Epoch: 65\n",
      "Epoch 65: Loss 1\n",
      "Epoch: 66\n",
      "Epoch 66: Loss 1\n",
      "Epoch: 67\n",
      "Epoch 67: Loss 1\n",
      "Epoch: 68\n",
      "Epoch 68: Loss 1\n",
      "Epoch: 69\n",
      "Epoch 69: Loss 1\n",
      "Epoch: 70\n",
      "Epoch 70: Loss 1\n",
      "Epoch: 71\n",
      "Epoch 42: Loss 0.9274755596441182\n",
      "Epoch: 43\n",
      "Epoch 71: Loss 1\n",
      "Epoch: 72\n",
      "Epoch 72: Loss 1\n",
      "Epoch: 73\n",
      "Epoch 73: Loss 1\n",
      "Epoch: 74\n",
      "Epoch 74: Loss 1\n",
      "Epoch: 75\n",
      "Epoch 75: Loss 1\n",
      "Epoch: 76\n",
      "Epoch 76: Loss 1\n",
      "Epoch: 77\n",
      "Epoch 77: Loss 1\n",
      "Epoch: 78\n",
      "Epoch 78: Loss 1\n",
      "Epoch: 79\n",
      "Epoch 79: Loss 1\n",
      "Epoch: 80\n",
      "Epoch 80: Loss 1\n",
      "Epoch: 81\n",
      "Epoch 81: Loss 1\n",
      "Epoch: 82\n",
      "Epoch 82: Loss 1\n",
      "Epoch: 83\n",
      "Epoch 83: Loss 1\n",
      "Epoch: 84\n",
      "Epoch 84: Loss 1\n",
      "Epoch: 85\n",
      "Epoch 85: Loss 1\n",
      "Epoch: 86\n",
      "Epoch 86: Loss 1\n",
      "Epoch: 87\n",
      "Epoch 87: Loss 1\n",
      "Epoch: 88\n",
      "Epoch 88: Loss 1\n",
      "Epoch: 89\n",
      "Epoch 89: Loss 1\n",
      "Epoch: 90\n",
      "Epoch 90: Loss 1\n",
      "Epoch: 91\n",
      "Epoch 91: Loss 1\n",
      "Epoch: 92\n",
      "Epoch 92: Loss 1\n",
      "Epoch: 93\n",
      "Epoch 93: Loss 1\n",
      "Epoch: 94\n",
      "Epoch 94: Loss 1\n",
      "Epoch: 95\n",
      "Epoch 95: Loss 1\n",
      "Epoch: 96\n",
      "Epoch 96: Loss 1\n",
      "Epoch: 97\n",
      "Epoch 97: Loss 1\n",
      "Epoch: 98\n",
      "Epoch 35: Loss 0.9274755596441182\n",
      "Epoch: 36\n",
      "Epoch 98: Loss 1\n",
      "Epoch: 99\n",
      "Epoch 99: Loss 1\n",
      "Epoch 41: Loss 0.9274755596441182\n",
      "Epoch: 42\n",
      "Epoch 43: Loss 0.9274755596441182\n",
      "Epoch: 44\n",
      "Epoch 36: Loss 0.9274755596441182\n",
      "Epoch: 37\n",
      "Epoch 42: Loss 0.9274755596441182\n",
      "Epoch: 43\n",
      "Epoch 44: Loss 0.9274755596441182\n",
      "Epoch: 45\n",
      "Epoch 37: Loss 0.9274755596441182\n",
      "Epoch: 38\n",
      "Epoch 45: Loss 0.9274755596441182\n",
      "Epoch: 46\n",
      "Epoch 43: Loss 0.9274755596441182\n",
      "Epoch: 44\n",
      "Epoch 38: Loss 0.9274755596441182\n",
      "Epoch: 39\n",
      "Epoch 46: Loss 0.9274755596441182\n",
      "Epoch: 47\n",
      "Epoch 44: Loss 0.9274755596441182\n",
      "Epoch: 45\n",
      "Epoch 39: Loss 0.9274755596441182\n",
      "Epoch: 40\n",
      "Epoch 47: Loss 0.9274755596441182\n",
      "Epoch: 48\n",
      "Epoch 45: Loss 0.9274755596441182\n",
      "Epoch: 46\n",
      "Epoch 48: Loss 0.9274755596441182\n",
      "Epoch: 49\n",
      "Epoch 40: Loss 0.9274755596441182\n",
      "Epoch: 41\n",
      "Epoch 46: Loss 0.9274755596441182\n",
      "Epoch: 47\n",
      "Epoch 49: Loss 0.9274755596441182\n",
      "Epoch: 50\n",
      "Epoch 47: Loss 0.9274755596441182\n",
      "Epoch: 48\n",
      "Epoch 41: Loss 0.9274755596441182\n",
      "Epoch: 42\n",
      "Epoch 50: Loss 0.9274755596441182\n",
      "Epoch: 51\n",
      "Epoch 48: Loss 0.9274755596441182\n",
      "Epoch: 49\n",
      "Epoch 42: Loss 0.9274755596441182\n",
      "Epoch: 43\n",
      "Epoch 51: Loss 0.9274755596441182\n",
      "Epoch: 52\n",
      "Epoch 49: Loss 0.9274755596441182\n",
      "Epoch: 50\n",
      "Epoch 43: Loss 0.9274755596441182\n",
      "Epoch: 44\n",
      "Epoch 52: Loss 0.9274755596441182\n",
      "Epoch: 53\n",
      "Epoch 50: Loss 0.9274755596441182\n",
      "Epoch: 51\n",
      "Epoch 44: Loss 0.9274755596441182\n",
      "Epoch: 45\n",
      "Epoch 53: Loss 0.9274755596441182\n",
      "Epoch: 54\n",
      "Epoch 51: Loss 0.9274755596441182\n",
      "Epoch: 52\n",
      "Epoch 54: Loss 0.9274755596441182\n",
      "Epoch: 55\n",
      "Epoch 45: Loss 0.9274755596441182\n",
      "Epoch: 46\n",
      "Epoch 52: Loss 0.9274755596441182\n",
      "Epoch: 53\n",
      "Epoch 55: Loss 0.9274755596441182\n",
      "Epoch: 56\n",
      "Epoch 46: Loss 0.9274755596441182\n",
      "Epoch: 47\n",
      "Epoch 53: Loss 0.9274755596441182\n",
      "Epoch: 54\n",
      "Epoch 56: Loss 0.9274755596441182\n",
      "Epoch: 57\n",
      "Epoch 54: Loss 0.9274755596441182\n",
      "Epoch: 55\n",
      "Epoch 47: Loss 0.9274755596441182\n",
      "Epoch: 48\n",
      "Epoch 57: Loss 0.9274755596441182\n",
      "Epoch: 58\n",
      "Epoch 55: Loss 0.9274755596441182\n",
      "Epoch: 56\n",
      "Epoch 48: Loss 0.9274755596441182\n",
      "Epoch: 49\n",
      "Epoch 58: Loss 0.9274755596441182\n",
      "Epoch: 59\n",
      "Epoch 56: Loss 0.9274755596441182\n",
      "Epoch: 57\n",
      "Epoch 49: Loss 0.9274755596441182\n",
      "Epoch: 50\n",
      "Epoch 59: Loss 0.9274755596441182\n",
      "Epoch: 60\n",
      "Epoch 57: Loss 0.9274755596441182\n",
      "Epoch: 58\n",
      "Epoch 60: Loss 0.9274755596441182\n",
      "Epoch: 61\n",
      "Epoch 50: Loss 0.9274755596441182\n",
      "Epoch: 51\n",
      "Epoch 58: Loss 0.9274755596441182\n",
      "Epoch: 59\n",
      "Epoch 61: Loss 0.9274755596441182\n",
      "Epoch: 62\n",
      "Epoch 51: Loss 0.9274755596441182\n",
      "Epoch: 52\n",
      "Epoch 59: Loss 0.9274755596441182\n",
      "Epoch: 60\n",
      "Epoch 62: Loss 0.9274755596441182\n",
      "Epoch: 63\n",
      "Epoch 52: Loss 0.9274755596441182\n",
      "Epoch: 53\n",
      "Epoch 60: Loss 0.9274755596441182\n",
      "Epoch: 61\n",
      "Epoch 63: Loss 0.9274755596441182\n",
      "Epoch: 64\n",
      "Epoch 53: Loss 0.9274755596441182\n",
      "Epoch: 54\n",
      "Epoch 61: Loss 0.9274755596441182\n",
      "Epoch: 62\n",
      "Epoch 64: Loss 0.9274755596441182\n",
      "Epoch: 65\n",
      "Epoch 62: Loss 0.9274755596441182\n",
      "Epoch: 63\n",
      "Epoch 54: Loss 0.9274755596441182\n",
      "Epoch: 55\n",
      "Epoch 65: Loss 0.9274755596441182\n",
      "Epoch: 66\n",
      "Epoch 63: Loss 0.9274755596441182\n",
      "Epoch: 64\n",
      "Epoch 66: Loss 0.9274755596441182\n",
      "Epoch: 67\n",
      "Epoch 55: Loss 0.9274755596441182\n",
      "Epoch: 56\n",
      "Epoch 64: Loss 0.9274755596441182\n",
      "Epoch: 65\n",
      "Epoch 67: Loss 0.9274755596441182\n",
      "Epoch: 68\n",
      "Epoch 56: Loss 0.9274755596441182\n",
      "Epoch: 57\n",
      "Epoch 65: Loss 0.9274755596441182\n",
      "Epoch: 66\n",
      "Epoch 68: Loss 0.9274755596441182\n",
      "Epoch: 69\n",
      "Epoch 57: Loss 0.9274755596441182\n",
      "Epoch: 58\n",
      "Epoch 66: Loss 0.9274755596441182\n",
      "Epoch: 67\n",
      "Epoch 69: Loss 0.9274755596441182\n",
      "Epoch: 70\n",
      "Epoch 58: Loss 0.9274755596441182\n",
      "Epoch: 59\n",
      "Epoch 67: Loss 0.9274755596441182\n",
      "Epoch: 68\n",
      "Epoch 70: Loss 0.9274755596441182\n",
      "Epoch: 71\n",
      "Epoch 59: Loss 0.9274755596441182\n",
      "Epoch: 60\n",
      "Epoch 68: Loss 0.9274755596441182\n",
      "Epoch: 69\n",
      "Epoch 71: Loss 0.9274755596441182\n",
      "Epoch: 72\n",
      "Epoch 69: Loss 0.9274755596441182\n",
      "Epoch: 70\n",
      "Epoch 60: Loss 0.9274755596441182\n",
      "Epoch: 61\n",
      "Epoch 72: Loss 0.9274755596441182\n",
      "Epoch: 73\n",
      "Epoch 73: Loss 0.9274755596441182\n",
      "Epoch: 74\n",
      "Epoch 70: Loss 0.9274755596441182\n",
      "Epoch: 71\n",
      "Epoch 61: Loss 0.9274755596441182\n",
      "Epoch: 62\n",
      "Epoch 74: Loss 0.9274755596441182\n",
      "Epoch: 75\n",
      "Epoch 71: Loss 0.9274755596441182\n",
      "Epoch: 72\n",
      "Epoch 62: Loss 0.9274755596441182\n",
      "Epoch: 63\n",
      "Epoch 75: Loss 0.9274755596441182\n",
      "Epoch: 76\n",
      "Epoch 72: Loss 0.9274755596441182\n",
      "Epoch: 73\n",
      "Epoch 63: Loss 0.9274755596441182\n",
      "Epoch: 64\n",
      "Epoch 76: Loss 0.9274755596441182\n",
      "Epoch: 77\n",
      "Epoch 73: Loss 0.9274755596441182\n",
      "Epoch: 74\n",
      "Epoch 64: Loss 0.9274755596441182\n",
      "Epoch: 65\n",
      "Epoch 77: Loss 0.9274755596441182\n",
      "Epoch: 78\n",
      "Epoch 74: Loss 0.9274755596441182\n",
      "Epoch: 75\n",
      "Epoch 65: Loss 0.9274755596441182\n",
      "Epoch: 66\n",
      "Epoch 78: Loss 0.9274755596441182\n",
      "Epoch: 79\n",
      "Epoch 75: Loss 0.9274755596441182\n",
      "Epoch: 76\n",
      "Epoch 79: Loss 0.9274755596441182\n",
      "Epoch: 80\n",
      "Epoch 66: Loss 0.9274755596441182\n",
      "Epoch: 67\n",
      "Epoch 76: Loss 0.9274755596441182\n",
      "Epoch: 77\n",
      "Epoch 80: Loss 0.9274755596441182\n",
      "Epoch: 81\n",
      "Epoch 77: Loss 0.9274755596441182\n",
      "Epoch: 78\n",
      "Epoch 67: Loss 0.9274755596441182\n",
      "Epoch: 68\n",
      "Epoch 81: Loss 0.9274755596441182\n",
      "Epoch: 82\n",
      "Epoch 78: Loss 0.9274755596441182\n",
      "Epoch: 79\n",
      "Epoch 68: Loss 0.9274755596441182\n",
      "Epoch: 69\n",
      "Epoch 82: Loss 0.9274755596441182\n",
      "Epoch: 83\n",
      "Epoch 79: Loss 0.9274755596441182\n",
      "Epoch: 80\n",
      "Epoch 69: Loss 0.9274755596441182\n",
      "Epoch: 70\n",
      "Epoch 83: Loss 0.9274755596441182\n",
      "Epoch: 84\n",
      "Epoch 80: Loss 0.9274755596441182\n",
      "Epoch: 81\n",
      "Epoch 70: Loss 0.9274755596441182\n",
      "Epoch: 71\n",
      "Epoch 84: Loss 0.9274755596441182\n",
      "Epoch: 85\n",
      "Epoch 81: Loss 0.9274755596441182\n",
      "Epoch: 82\n",
      "Epoch 85: Loss 0.9274755596441182\n",
      "Epoch: 86\n",
      "Epoch 71: Loss 0.9274755596441182\n",
      "Epoch: 72\n",
      "Epoch 82: Loss 0.9274755596441182\n",
      "Epoch: 83\n",
      "Epoch 86: Loss 0.9274755596441182\n",
      "Epoch: 87\n",
      "Epoch 72: Loss 0.9274755596441182\n",
      "Epoch: 73\n",
      "Epoch 83: Loss 0.9274755596441182\n",
      "Epoch: 84\n",
      "Epoch 87: Loss 0.9274755596441182\n",
      "Epoch: 88\n",
      "Epoch 73: Loss 0.9274755596441182\n",
      "Epoch: 74\n",
      "Epoch 84: Loss 0.9274755596441182\n",
      "Epoch: 85\n",
      "Epoch 88: Loss 0.9274755596441182\n",
      "Epoch: 89\n",
      "Epoch 85: Loss 0.9274755596441182\n",
      "Epoch: 86\n",
      "Epoch 74: Loss 0.9274755596441182\n",
      "Epoch: 75\n",
      "Epoch 89: Loss 0.9274755596441182\n",
      "Epoch: 90\n",
      "Epoch 86: Loss 0.9274755596441182\n",
      "Epoch: 87\n",
      "Epoch 75: Loss 0.9274755596441182\n",
      "Epoch: 76\n",
      "Epoch 90: Loss 0.9274755596441182\n",
      "Epoch: 91\n",
      "Epoch 87: Loss 0.9274755596441182\n",
      "Epoch: 88\n",
      "Epoch 91: Loss 0.9274755596441182\n",
      "Epoch: 92\n",
      "Epoch 76: Loss 0.9274755596441182\n",
      "Epoch: 77\n",
      "Epoch 88: Loss 0.9274755596441182\n",
      "Epoch: 89\n",
      "Epoch 92: Loss 0.9274755596441182\n",
      "Epoch: 93\n",
      "Epoch 77: Loss 0.9274755596441182\n",
      "Epoch: 78\n",
      "Epoch 89: Loss 0.9274755596441182\n",
      "Epoch: 90\n",
      "Epoch 93: Loss 0.9274755596441182\n",
      "Epoch: 94\n",
      "Epoch 78: Loss 0.9274755596441182\n",
      "Epoch: 79\n",
      "Epoch 90: Loss 0.9274755596441182\n",
      "Epoch: 91\n",
      "Epoch 94: Loss 0.9274755596441182\n",
      "Epoch: 95\n",
      "Epoch 79: Loss 0.9274755596441182\n",
      "Epoch: 80\n",
      "Epoch 91: Loss 0.9274755596441182\n",
      "Epoch: 92\n",
      "Epoch 95: Loss 0.9274755596441182\n",
      "Epoch: 96\n",
      "Epoch 92: Loss 0.9274755596441182\n",
      "Epoch: 93\n",
      "Epoch 80: Loss 0.9274755596441182\n",
      "Epoch: 81\n",
      "Epoch 96: Loss 0.9274755596441182\n",
      "Epoch: 97\n",
      "Epoch 93: Loss 0.9274755596441182\n",
      "Epoch: 94\n",
      "Epoch 97: Loss 0.9274755596441182\n",
      "Epoch: 98\n",
      "Epoch 81: Loss 0.9274755596441182\n",
      "Epoch: 82\n",
      "Epoch 94: Loss 0.9274755596441182\n",
      "Epoch: 95\n",
      "Epoch 98: Loss 0.9274755596441182\n",
      "Epoch: 99\n",
      "Epoch 82: Loss 0.9274755596441182\n",
      "Epoch: 83\n",
      "Epoch 95: Loss 0.9274755596441182\n",
      "Epoch: 96\n",
      "Epoch 99: Loss 0.9274755596441182\n",
      "Epoch 83: Loss 0.9274755596441182\n",
      "Epoch: 84\n",
      "Epoch 96: Loss 0.9274755596441182\n",
      "Epoch: 97\n",
      "Epoch 84: Loss 0.9274755596441182\n",
      "Epoch: 85\n",
      "Epoch 97: Loss 0.9274755596441182\n",
      "Epoch: 98\n",
      "Epoch 85: Loss 0.9274755596441182\n",
      "Epoch: 86\n",
      "Epoch 98: Loss 0.9274755596441182\n",
      "Epoch: 99\n",
      "Epoch 86: Loss 0.9274755596441182\n",
      "Epoch: 87\n",
      "Epoch 99: Loss 0.9274755596441182\n",
      "Epoch 87: Loss 0.9274755596441182\n",
      "Epoch: 88\n",
      "Epoch 88: Loss 0.9274755596441182\n",
      "Epoch: 89\n",
      "Epoch 89: Loss 0.9274755596441182\n",
      "Epoch: 90\n",
      "Epoch 90: Loss 0.9274755596441182\n",
      "Epoch: 91\n",
      "Epoch 91: Loss 0.9274755596441182\n",
      "Epoch: 92\n",
      "Epoch 92: Loss 0.9274755596441182\n",
      "Epoch: 93\n",
      "Epoch 93: Loss 0.9274755596441182\n",
      "Epoch: 94\n",
      "Epoch 94: Loss 0.9274755596441182\n",
      "Epoch: 95\n",
      "Epoch 95: Loss 0.9274755596441182\n",
      "Epoch: 96\n",
      "Epoch 96: Loss 0.9274755596441182\n",
      "Epoch: 97\n",
      "Epoch 97: Loss 0.9274755596441182\n",
      "Epoch: 98\n",
      "Epoch 98: Loss 0.9274755596441182\n",
      "Epoch: 99\n",
      "Epoch 99: Loss 0.9274755596441182\n",
      "Generation: 0\n",
      "Saving model to: ./trained_model with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }, LayerShape { layer_type: Dense { input_size: 10, output_size: 10 }, activation: ReLU }] }\n",
      "Epoch: 0\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 196 }, activation: ReLU }, LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Tanh }] }\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at src/neural/mat/matrix.rs:70:19:\n",
      "index out of bounds: the len is 1960 but the index is 1960\n",
      "stack backtrace:\n",
      "   0: rust_begin_unwind\n",
      "   1: core::panicking::panic_fmt\n",
      "   2: core::panicking::panic_bounds_check\n",
      "   3: <learn::neural::layer::dense_layer::DenseLayer as learn::neural::layer::layer_trait::Layer>::forward\n",
      "   4: learn::neural::nn::neuralnet::NeuralNetwork::train\n",
      "   5: learn::neural::training::training_session::TrainingSession::train\n",
      "   6: <learn::gen::challenge::nn_challenge::NeuralNetworkChallenge as learn::evol::evolution::challenge::Challenge<learn::gen::pheno::nn_pheno::NeuralNetworkPhenotype>>::score\n",
      "   7: core::ops::function::impls::<impl core::ops::function::FnMut<A> for &F>::call_mut\n",
      "   8: rayon_core::join::join_context::{{closure}}\n",
      "   9: rayon::iter::plumbing::bridge_producer_consumer::helper\n",
      "  10: <rayon_core::job::StackJob<L,F,R> as rayon_core::job::Job>::execute\n",
      "  11: rayon_core::registry::WorkerThread::wait_until_cold\n",
      "  12: rayon_core::registry::ThreadBuilder::run\n",
      "note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.2274510580259939\n",
      "Epoch: 1\n",
      "Epoch 0: Loss 0.9272043614643315\n",
      "Epoch: 1\n",
      "Epoch 0: Loss 7.774121030104593\n",
      "Epoch: 1\n",
      "Epoch 1: Loss 0.2274285827091618\n",
      "Epoch: 2\n",
      "Epoch 1: Loss 0.9274755596441182\n",
      "Epoch: 2\n",
      "Epoch 1: Loss 7.711012681434714\n",
      "Epoch: 2\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 564 }, activation: Tanh }, LayerShape { layer_type: Dense { input_size: 564, output_size: 196 }, activation: ReLU }, LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }, LayerShape { layer_type: Dense { input_size: 10, output_size: 196 }, activation: ReLU }, LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Epoch 2: Loss 0.22740641955216312\n",
      "Epoch: 3\n",
      "Epoch 2: Loss 7.707663379706051\n",
      "Epoch: 3\n",
      "Epoch 2: Loss 0.9274755596441182\n",
      "Epoch: 3\n",
      "Epoch 3: Loss 0.22738456161369386\n",
      "Epoch: 4\n",
      "Epoch 3: Loss 7.685607714748273\n",
      "Epoch: 4\n",
      "Epoch 3: Loss 0.9274755596441182\n",
      "Epoch: 4\n",
      "Epoch 4: Loss 0.22736300219477537\n",
      "Epoch: 5\n",
      "Epoch 4: Loss 7.686385870040158\n",
      "Epoch: 5\n",
      "Epoch 5: Loss 0.22734173478888087\n",
      "Epoch: 6\n",
      "Epoch 4: Loss 0.9274755596441182\n",
      "Epoch: 5\n",
      "Epoch 5: Loss 7.678934109065266\n",
      "Epoch: 6\n",
      "Epoch 6: Loss 0.2273207530386601\n",
      "Epoch: 7\n",
      "Epoch 6: Loss 7.696864624490966\n",
      "Epoch: 7\n",
      "Epoch 5: Loss 0.9274755596441182\n",
      "Epoch: 6\n",
      "Epoch 7: Loss 0.22730005070032233\n",
      "Epoch: 8\n",
      "Epoch 7: Loss 7.693155844681857\n",
      "Epoch: 8\n",
      "Epoch 8: Loss 0.22727962161575307\n",
      "Epoch: 9\n",
      "Epoch 6: Loss 0.9274755596441182\n",
      "Epoch: 7\n",
      "Epoch 8: Loss 7.7102801441352575\n",
      "Epoch: 9\n",
      "Epoch 9: Loss 0.2272594596924186\n",
      "Epoch: 10\n",
      "Epoch 9: Loss 7.68881844642779\n",
      "Epoch: 10\n",
      "Epoch 7: Loss 0.9274755596441182\n",
      "Epoch: 8\n",
      "Epoch 10: Loss 0.22723955889049963\n",
      "Epoch: 11\n",
      "Epoch 10: Loss 7.69682584472811\n",
      "Epoch: 11\n",
      "Epoch 11: Loss 0.22721991321665383\n",
      "Epoch: 12\n",
      "Epoch 8: Loss 0.9274755596441182\n",
      "Epoch: 9\n",
      "Epoch 11: Loss 7.697470664970443\n",
      "Epoch: 12\n",
      "Epoch 12: Loss 0.22720051672336836\n",
      "Epoch: 13\n",
      "Epoch 12: Loss 7.698459383178528\n",
      "Epoch: 13\n",
      "Epoch 9: Loss 0.9274755596441182\n",
      "Epoch: 10\n",
      "Epoch 13: Loss 0.2271813635129094\n",
      "Epoch: 14\n",
      "Epoch 13: Loss 7.7120133112490965\n",
      "Epoch: 14\n",
      "Epoch 10: Loss 0.9274755596441182\n",
      "Epoch: 11\n",
      "Epoch 14: Loss 0.22716244774483488\n",
      "Epoch: 15\n",
      "Epoch 14: Loss 7.717768554021778\n",
      "Epoch: 15\n",
      "Epoch 15: Loss 0.22714376364593894\n",
      "Epoch: 16\n",
      "Epoch 11: Loss 0.9274755596441182\n",
      "Epoch: 12\n",
      "Epoch 15: Loss 7.722537597401725\n",
      "Epoch: 16\n",
      "Epoch 16: Loss 0.2271253055218498\n",
      "Epoch: 17\n",
      "Epoch 16: Loss 7.710055806266306\n",
      "Epoch: 17\n",
      "Epoch 12: Loss 0.9274755596441182\n",
      "Epoch: 13\n",
      "Epoch 17: Loss 0.22710706776944375\n",
      "Epoch: 18\n",
      "Epoch 17: Loss 7.701699877684518\n",
      "Epoch: 18\n",
      "Epoch 18: Loss 0.22708904488939222\n",
      "Epoch: 19\n",
      "Epoch 13: Loss 0.9274755596441182\n",
      "Epoch: 14\n",
      "Epoch 18: Loss 7.695885842489733\n",
      "Epoch: 19\n",
      "Epoch 19: Loss 0.22707123149840672\n",
      "Epoch: 20\n",
      "Epoch 19: Loss 7.700032523662398\n",
      "Epoch: 20\n",
      "Epoch 14: Loss 0.9274755596441182\n",
      "Epoch: 15\n",
      "Epoch 20: Loss 0.22705362234067422\n",
      "Epoch: 21\n",
      "Epoch 20: Loss 7.702399752377158\n",
      "Epoch: 21\n",
      "Epoch 15: Loss 0.9274755596441182\n",
      "Epoch: 16\n",
      "Epoch 21: Loss 0.22703621229828527\n",
      "Epoch: 22\n",
      "Epoch 21: Loss 7.693828026229319\n",
      "Epoch: 22\n",
      "Epoch 22: Loss 0.2270189964003377\n",
      "Epoch: 23\n",
      "Epoch 16: Loss 0.9274755596441182\n",
      "Epoch: 17\n",
      "Epoch 22: Loss 7.698574449267835\n",
      "Epoch: 23\n",
      "Epoch 23: Loss 0.22700196983068294\n",
      "Epoch: 24\n",
      "Epoch 23: Loss 7.694792584962588\n",
      "Epoch: 24\n",
      "Epoch 17: Loss 0.9274755596441182\n",
      "Epoch: 18\n",
      "Epoch 24: Loss 0.22698512793405087\n",
      "Epoch: 25\n",
      "Epoch 24: Loss 7.717561119544632\n",
      "Epoch: 25\n",
      "Epoch 25: Loss 0.22696846622078698\n",
      "Epoch: 26\n",
      "Epoch 18: Loss 0.9274755596441182\n",
      "Epoch: 19\n",
      "Epoch 25: Loss 7.705788602216439\n",
      "Epoch: 26\n",
      "Epoch 26: Loss 0.22695198036998598\n",
      "Epoch: 27\n",
      "Epoch 19: Loss 0.9274755596441182\n",
      "Epoch: 20\n",
      "Epoch 26: Loss 7.700296408716514\n",
      "Epoch: 27\n",
      "Epoch 27: Loss 0.22693566623123268\n",
      "Epoch: 28\n",
      "Epoch 27: Loss 7.7051057895575905\n",
      "Epoch: 28\n",
      "Epoch 20: Loss 0.9274755596441182\n",
      "Epoch: 21\n",
      "Epoch 28: Loss 0.22691951982498945\n",
      "Epoch: 29\n",
      "Epoch 28: Loss 7.68523288525804\n",
      "Epoch: 29\n",
      "Epoch 29: Loss 0.2269035373417659\n",
      "Epoch: 30\n",
      "Epoch 21: Loss 0.9274755596441182\n",
      "Epoch: 22\n",
      "Epoch 29: Loss 7.693484580340254\n",
      "Epoch: 30\n",
      "Epoch 30: Loss 0.22688771514014175\n",
      "Epoch: 31\n",
      "Epoch 30: Loss 7.703412679070756\n",
      "Epoch: 31\n",
      "Epoch 22: Loss 0.9274755596441182\n",
      "Epoch: 23\n",
      "Epoch 31: Loss 0.22687204974389616\n",
      "Epoch: 32\n",
      "Epoch 31: Loss 7.705686462934632\n",
      "Epoch: 32\n",
      "Epoch 32: Loss 0.22685653783825205\n",
      "Epoch: 33\n",
      "Epoch 23: Loss 0.9274755596441182\n",
      "Epoch: 24\n",
      "Epoch 32: Loss 7.692530989704046\n",
      "Epoch: 33\n",
      "Epoch 33: Loss 0.22684117626542555\n",
      "Epoch: 34\n",
      "Epoch 33: Loss 7.694076318874012\n",
      "Epoch: 34\n",
      "Epoch 24: Loss 0.9274755596441182\n",
      "Epoch: 25\n",
      "Epoch 34: Loss 0.22682596201965294\n",
      "Epoch: 35\n",
      "Epoch 34: Loss 7.691093282848427\n",
      "Epoch: 35\n",
      "Epoch 35: Loss 0.22681089224174109\n",
      "Epoch: 36\n",
      "Epoch 25: Loss 0.9274755596441182\n",
      "Epoch: 26\n",
      "Epoch 35: Loss 7.701139775282956\n",
      "Epoch: 36\n",
      "Epoch 36: Loss 0.2267959642131965\n",
      "Epoch: 37\n",
      "Epoch 26: Loss 0.9274755596441182\n",
      "Epoch: 27\n",
      "Epoch 37: Loss 0.22678117535022874\n",
      "Epoch: 38\n",
      "Epoch 36: Loss 7.700307477245446\n",
      "Epoch: 37\n",
      "Epoch 38: Loss 0.22676652319742874\n",
      "Epoch: 39\n",
      "Epoch 37: Loss 7.701649936133426\n",
      "Epoch: 38\n",
      "Epoch 27: Loss 0.9274755596441182\n",
      "Epoch: 28\n",
      "Epoch 39: Loss 0.22675200542146265\n",
      "Epoch: 40\n",
      "Epoch 38: Loss 7.6951004934320615\n",
      "Epoch: 39\n",
      "Epoch 28: Loss 0.9274755596441182\n",
      "Epoch: 29\n",
      "Epoch 40: Loss 0.2267376198046028\n",
      "Epoch: 41\n",
      "Epoch 39: Loss 7.72048249388519\n",
      "Epoch: 40\n",
      "Epoch 41: Loss 0.2267233642383319\n",
      "Epoch: 42\n",
      "Epoch 29: Loss 0.9274755596441182\n",
      "Epoch: 30\n",
      "Epoch 40: Loss 7.695420184058985\n",
      "Epoch: 41\n",
      "Epoch 42: Loss 0.22670923671694876\n",
      "Epoch: 43\n",
      "Epoch 41: Loss 7.7162156143983385\n",
      "Epoch: 42\n",
      "Epoch 30: Loss 0.9274755596441182\n",
      "Epoch: 31\n",
      "Epoch 43: Loss 0.22669523533124847\n",
      "Epoch: 44\n",
      "Epoch 42: Loss 7.7079137024846665\n",
      "Epoch: 43\n",
      "Epoch 44: Loss 0.22668135826231303\n",
      "Epoch: 45\n",
      "Epoch 31: Loss 0.9274755596441182\n",
      "Epoch: 32\n",
      "Epoch 43: Loss 7.69919339729564\n",
      "Epoch: 44\n",
      "Epoch 45: Loss 0.2266676037754394\n",
      "Epoch: 46\n",
      "Epoch 44: Loss 7.6923691001339\n",
      "Epoch: 45\n",
      "Epoch 32: Loss 0.9274755596441182\n",
      "Epoch: 33\n",
      "Epoch 46: Loss 0.22665397021415645\n",
      "Epoch: 47\n",
      "Epoch 45: Loss 7.701513447503123\n",
      "Epoch: 46\n",
      "Epoch 47: Loss 0.22664045599453006\n",
      "Epoch: 48\n",
      "Epoch 33: Loss 0.9274755596441182\n",
      "Epoch: 34\n",
      "Epoch 46: Loss 7.697094169967011\n",
      "Epoch: 47\n",
      "Epoch 48: Loss 0.22662705959953733\n",
      "Epoch: 49\n",
      "Epoch 34: Loss 0.9274755596441182\n",
      "Epoch: 35\n",
      "Epoch 47: Loss 7.701538673534724\n",
      "Epoch: 48\n",
      "Epoch 49: Loss 0.22661377957370407\n",
      "Epoch: 50\n",
      "Epoch 48: Loss 7.700086182738969\n",
      "Epoch: 49\n",
      "Epoch 35: Loss 0.9274755596441182\n",
      "Epoch: 36\n",
      "Epoch 50: Loss 0.22660061451796598\n",
      "Epoch: 51\n",
      "Epoch 49: Loss 7.6917142739839335\n",
      "Epoch: 50\n",
      "Epoch 51: Loss 0.2265875630847104\n",
      "Epoch: 52\n",
      "Epoch 36: Loss 0.9274755596441182\n",
      "Epoch: 37\n",
      "Epoch 50: Loss 7.694252132724977\n",
      "Epoch: 51\n",
      "Epoch 52: Loss 0.2265746239731025\n",
      "Epoch: 53\n",
      "Epoch 51: Loss 7.70258197437321\n",
      "Epoch: 52\n",
      "Epoch 37: Loss 0.9274755596441182\n",
      "Epoch: 38\n",
      "Epoch 53: Loss 0.22656179592465994\n",
      "Epoch: 54\n",
      "Epoch 52: Loss 7.701521016263409\n",
      "Epoch: 53\n",
      "Epoch 54: Loss 0.22654907771905397\n",
      "Epoch: 55\n",
      "Epoch 38: Loss 0.9274755596441182\n",
      "Epoch: 39\n",
      "Epoch 53: Loss 7.700724351824461\n",
      "Epoch: 54\n",
      "Epoch 55: Loss 0.2265364681701905\n",
      "Epoch: 56\n",
      "Epoch 56: Loss 0.2265239661225533\n",
      "Epoch: 57\n",
      "Epoch 39: Loss 0.9274755596441182\n",
      "Epoch: 40\n",
      "Epoch 54: Loss 7.700368716672939\n",
      "Epoch: 55\n",
      "Epoch 57: Loss 0.22651157044783557\n",
      "Epoch: 58\n",
      "Epoch 55: Loss 7.680357810805012\n",
      "Epoch: 56\n",
      "Epoch 40: Loss 0.9274755596441182\n",
      "Epoch: 41\n",
      "Epoch 58: Loss 0.22649928004178857\n",
      "Epoch: 59\n",
      "Epoch 56: Loss 7.701493989082617\n",
      "Epoch: 57\n",
      "Epoch 41: Loss 0.9274755596441182\n",
      "Epoch: 42\n",
      "Epoch 59: Loss 0.226487093821408\n",
      "Epoch: 60\n",
      "Epoch 57: Loss 7.700749730789881\n",
      "Epoch: 58\n",
      "Epoch 60: Loss 0.2264750107222679\n",
      "Epoch: 61\n",
      "Epoch 42: Loss 0.9274755596441182\n",
      "Epoch: 43\n",
      "Epoch 58: Loss 7.695960090722878\n",
      "Epoch: 59\n",
      "Epoch 61: Loss 0.2264630296962446\n",
      "Epoch: 62\n",
      "Epoch 59: Loss 7.710835957268408\n",
      "Epoch: 60\n",
      "Epoch 43: Loss 0.9274755596441182\n",
      "Epoch: 44\n",
      "Epoch 62: Loss 0.22645114970933972\n",
      "Epoch: 63\n",
      "Epoch 60: Loss 7.714117052754353\n",
      "Epoch: 61\n",
      "Epoch 63: Loss 0.22643936973988096\n",
      "Epoch: 64\n",
      "Epoch 44: Loss 0.9274755596441182\n",
      "Epoch: 45\n",
      "Epoch 61: Loss 7.702833373656461\n",
      "Epoch: 62\n",
      "Epoch 64: Loss 0.2264276887767948\n",
      "Epoch: 65\n",
      "Epoch 62: Loss 7.690542434978592\n",
      "Epoch: 63\n",
      "Epoch 45: Loss 0.9274755596441182\n",
      "Epoch: 46\n",
      "Epoch 65: Loss 0.22641610581817143\n",
      "Epoch: 66\n",
      "Epoch 63: Loss 7.703086906082843\n",
      "Epoch: 64\n",
      "Epoch 66: Loss 0.2264046198700275\n",
      "Epoch: 67\n",
      "Epoch 46: Loss 0.9274755596441182\n",
      "Epoch: 47\n",
      "Epoch 64: Loss 7.719775308908355\n",
      "Epoch: 65\n",
      "Epoch 67: Loss 0.22639322994517413\n",
      "Epoch: 68\n",
      "Epoch 47: Loss 0.9274755596441182\n",
      "Epoch: 48\n",
      "Epoch 65: Loss 7.705006195735586\n",
      "Epoch: 66\n",
      "Epoch 68: Loss 0.2263819350623021\n",
      "Epoch: 69\n",
      "Epoch 66: Loss 7.710824208703878\n",
      "Epoch: 67\n",
      "Epoch 69: Loss 0.22637073424517254\n",
      "Epoch: 70\n",
      "Epoch 48: Loss 0.9274755596441182\n",
      "Epoch: 49\n",
      "Epoch 67: Loss 7.723573982852366\n",
      "Epoch: 68\n",
      "Epoch 70: Loss 0.22635962652194913\n",
      "Epoch: 71\n",
      "Epoch 49: Loss 0.9274755596441182\n",
      "Epoch: 50\n",
      "Epoch 68: Loss 7.68957453747801\n",
      "Epoch: 69\n",
      "Epoch 71: Loss 0.22634861092465902\n",
      "Epoch: 72\n",
      "Epoch 69: Loss 7.700194754807747\n",
      "Epoch: 70\n",
      "Epoch 50: Loss 0.9274755596441182\n",
      "Epoch: 51\n",
      "Epoch 72: Loss 0.22633768648867955\n",
      "Epoch: 73\n",
      "Epoch 70: Loss 7.694583962800051\n",
      "Epoch: 71\n",
      "Epoch 73: Loss 0.22632685225238505\n",
      "Epoch: 74\n",
      "Epoch 51: Loss 0.9274755596441182\n",
      "Epoch: 52\n",
      "Epoch 71: Loss 7.682917328377616\n",
      "Epoch: 72\n",
      "Epoch 74: Loss 0.22631610725682033\n",
      "Epoch: 75\n",
      "Epoch 52: Loss 0.9274755596441182\n",
      "Epoch: 53\n",
      "Epoch 72: Loss 7.694695498768603\n",
      "Epoch: 73\n",
      "Epoch 75: Loss 0.2263054505454406\n",
      "Epoch: 76\n",
      "Epoch 73: Loss 7.707380647904292\n",
      "Epoch: 74\n",
      "Epoch 76: Loss 0.22629488116390362\n",
      "Epoch: 77\n",
      "Epoch 53: Loss 0.9274755596441182\n",
      "Epoch: 54\n",
      "Epoch 77: Loss 0.22628439815987872\n",
      "Epoch: 78\n",
      "Epoch 74: Loss 7.690110977787001\n",
      "Epoch: 75\n",
      "Epoch 54: Loss 0.9274755596441182\n",
      "Epoch: 55\n",
      "Epoch 78: Loss 0.2262740005829226\n",
      "Epoch: 79\n",
      "Epoch 75: Loss 7.6805387474395665\n",
      "Epoch: 76\n",
      "Epoch 79: Loss 0.22626368748438322\n",
      "Epoch: 80\n",
      "Epoch 76: Loss 7.706243527576508\n",
      "Epoch: 77\n",
      "Epoch 55: Loss 0.9274755596441182\n",
      "Epoch: 56\n",
      "Epoch 80: Loss 0.22625345791726467\n",
      "Epoch: 81\n",
      "Epoch 77: Loss 7.703006409087984\n",
      "Epoch: 78\n",
      "Epoch 56: Loss 0.9274755596441182\n",
      "Epoch: 57\n",
      "Epoch 81: Loss 0.22624331093618688\n",
      "Epoch: 82\n",
      "Epoch 78: Loss 7.7025242837158965\n",
      "Epoch: 79\n",
      "Epoch 82: Loss 0.2262332455973182\n",
      "Epoch: 83\n",
      "Epoch 79: Loss 7.706112641686764\n",
      "Epoch: 80\n",
      "Epoch 57: Loss 0.9274755596441182\n",
      "Epoch: 58\n",
      "Epoch 83: Loss 0.22622326095835713\n",
      "Epoch: 84\n",
      "Epoch 80: Loss 7.701237596108908\n",
      "Epoch: 81\n",
      "Epoch 58: Loss 0.9274755596441182\n",
      "Epoch: 59\n",
      "Epoch 84: Loss 0.22621335607848916\n",
      "Epoch: 85\n",
      "Epoch 81: Loss 7.695700345147516\n",
      "Epoch: 82\n",
      "Epoch 85: Loss 0.2262035300183739\n",
      "Epoch: 86\n",
      "Epoch 59: Loss 0.9274755596441182\n",
      "Epoch: 60\n",
      "Epoch 82: Loss 7.707847339876314\n",
      "Epoch: 83\n",
      "Epoch 86: Loss 0.22619378184016625\n",
      "Epoch: 87\n",
      "Epoch 83: Loss 7.690240309445387\n",
      "Epoch: 84\n",
      "Epoch 60: Loss 0.9274755596441182\n",
      "Epoch: 61\n",
      "Epoch 87: Loss 0.2261841106075483\n",
      "Epoch: 88\n",
      "Epoch 84: Loss 7.698322199971373\n",
      "Epoch: 85\n",
      "Epoch 88: Loss 0.22617451538574943\n",
      "Epoch: 89\n",
      "Epoch 61: Loss 0.9274755596441182\n",
      "Epoch: 62\n",
      "Epoch 85: Loss 7.700127319071201\n",
      "Epoch: 86\n",
      "Epoch 89: Loss 0.2261649952416156\n",
      "Epoch: 90\n",
      "Epoch 86: Loss 7.700768452800805\n",
      "Epoch: 87\n",
      "Epoch 90: Loss 0.22615554924370215\n",
      "Epoch: 91\n",
      "Epoch 62: Loss 0.9274755596441182\n",
      "Epoch: 63\n",
      "Epoch 87: Loss 7.677092888361514\n",
      "Epoch: 88\n",
      "Epoch 91: Loss 0.22614617646238083\n",
      "Epoch: 92\n",
      "Epoch 63: Loss 0.9274755596441182\n",
      "Epoch: 64\n",
      "Epoch 88: Loss 7.702237837426569\n",
      "Epoch: 89\n",
      "Epoch 92: Loss 0.2261368759699354\n",
      "Epoch: 93\n",
      "Epoch 89: Loss 7.696730846026837\n",
      "Epoch: 90\n",
      "Epoch 64: Loss 0.9274755596441182\n",
      "Epoch: 65\n",
      "Epoch 93: Loss 0.22612764684077907\n",
      "Epoch: 94\n",
      "Epoch 90: Loss 7.709835716877756\n",
      "Epoch: 91\n",
      "Epoch 94: Loss 0.22611848815154098\n",
      "Epoch: 95\n",
      "Epoch 65: Loss 0.9274755596441182\n",
      "Epoch: 66\n",
      "Epoch 95: Loss 0.22610939898140514\n",
      "Epoch: 96\n",
      "Epoch 91: Loss 7.7042981269512145\n",
      "Epoch: 92\n",
      "Epoch 96: Loss 0.2261003784122206\n",
      "Epoch: 97\n",
      "Epoch 66: Loss 0.9274755596441182\n",
      "Epoch: 67\n",
      "Epoch 92: Loss 7.700029794836689\n",
      "Epoch: 93\n",
      "Epoch 97: Loss 0.22609142552883466\n",
      "Epoch: 98\n",
      "Epoch 93: Loss 7.687791470146845\n",
      "Epoch: 94\n",
      "Epoch 67: Loss 0.9274755596441182\n",
      "Epoch: 68\n",
      "Epoch 98: Loss 0.22608253941939604\n",
      "Epoch: 99\n",
      "Epoch 94: Loss 7.717473759096513\n",
      "Epoch: 95\n",
      "Epoch 99: Loss 0.22607371917565572\n",
      "Epoch 68: Loss 0.9274755596441182\n",
      "Epoch: 69\n",
      "Epoch 95: Loss 7.712983844504914\n",
      "Epoch: 96\n",
      "Epoch 0: Loss 0.927284966718291\n",
      "Epoch: 1\n",
      "Epoch 69: Loss 0.9274755596441182\n",
      "Epoch: 70\n",
      "Epoch 96: Loss 7.680937714247172\n",
      "Epoch: 97\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Epoch 97: Loss 7.703719340177133\n",
      "Epoch: 98\n",
      "Epoch 70: Loss 0.9274755596441182\n",
      "Epoch: 71\n",
      "Epoch 0: Loss 0.2274510580259939\n",
      "Epoch: 1\n",
      "Epoch 98: Loss 7.695924215316502\n",
      "Epoch: 99\n",
      "Epoch 1: Loss 0.2274285827091618\n",
      "Epoch: 2\n",
      "Epoch 71: Loss 0.9274755596441182\n",
      "Epoch: 72\n",
      "Epoch 99: Loss 7.705233066346382\n",
      "Epoch 2: Loss 0.22740641955216312\n",
      "Epoch: 3\n",
      "Epoch 72: Loss 0.9274755596441182\n",
      "Epoch: 73\n",
      "Epoch 3: Loss 0.22738456161369386\n",
      "Epoch: 4\n",
      "Epoch 4: Loss 0.22736300219477537\n",
      "Epoch: 5\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Epoch 5: Loss 0.22734173478888087\n",
      "Epoch: 6\n",
      "Epoch 73: Loss 0.9274755596441182\n",
      "Epoch: 74\n",
      "Epoch 0: Loss 0.2274510580259939\n",
      "Epoch: 1\n",
      "Epoch 6: Loss 0.2273207530386601\n",
      "Epoch: 7\n",
      "Epoch 74: Loss 0.9274755596441182\n",
      "Epoch: 75\n",
      "Epoch 1: Loss 0.2274285827091618\n",
      "Epoch: 2\n",
      "Epoch 7: Loss 0.22730005070032233\n",
      "Epoch: 8\n",
      "Epoch 2: Loss 0.22740641955216312\n",
      "Epoch: 3\n",
      "Epoch 8: Loss 0.22727962161575307\n",
      "Epoch: 9\n",
      "Epoch 75: Loss 0.9274755596441182\n",
      "Epoch: 76\n",
      "Epoch 3: Loss 0.22738456161369386\n",
      "Epoch: 4\n",
      "Epoch 9: Loss 0.2272594596924186\n",
      "Epoch: 10\n",
      "Epoch 4: Loss 0.22736300219477537\n",
      "Epoch: 5\n",
      "Epoch 76: Loss 0.9274755596441182\n",
      "Epoch: 77\n",
      "Epoch 10: Loss 0.22723955889049963\n",
      "Epoch: 11\n",
      "Epoch 5: Loss 0.22734173478888087\n",
      "Epoch: 6\n",
      "Epoch 77: Loss 0.9274755596441182\n",
      "Epoch: 78\n",
      "Epoch 11: Loss 0.22721991321665383\n",
      "Epoch: 12\n",
      "Epoch 6: Loss 0.2273207530386601\n",
      "Epoch: 7\n",
      "Epoch 12: Loss 0.22720051672336836\n",
      "Epoch: 13\n",
      "Epoch 78: Loss 0.9274755596441182\n",
      "Epoch: 79\n",
      "Epoch 7: Loss 0.22730005070032233\n",
      "Epoch: 8\n",
      "Epoch 13: Loss 0.2271813635129094\n",
      "Epoch: 14\n",
      "Epoch 8: Loss 0.22727962161575307\n",
      "Epoch: 9\n",
      "Epoch 79: Loss 0.9274755596441182\n",
      "Epoch: 80\n",
      "Epoch 14: Loss 0.22716244774483488\n",
      "Epoch: 15\n",
      "Epoch 9: Loss 0.2272594596924186\n",
      "Epoch: 10\n",
      "Epoch 15: Loss 0.22714376364593894\n",
      "Epoch: 16\n",
      "Epoch 80: Loss 0.9274755596441182\n",
      "Epoch: 81\n",
      "Epoch 10: Loss 0.22723955889049963\n",
      "Epoch: 11\n",
      "Epoch 16: Loss 0.2271253055218498\n",
      "Epoch: 17\n",
      "Epoch 11: Loss 0.22721991321665383\n",
      "Epoch: 12\n",
      "Epoch 81: Loss 0.9274755596441182\n",
      "Epoch: 82\n",
      "Epoch 17: Loss 0.22710706776944375\n",
      "Epoch: 18\n",
      "Epoch 12: Loss 0.22720051672336836\n",
      "Epoch: 13\n",
      "Epoch 13: Loss 0.2271813635129094\n",
      "Epoch: 14\n",
      "Epoch 82: Loss 0.9274755596441182\n",
      "Epoch: 83\n",
      "Epoch 18: Loss 0.22708904488939222\n",
      "Epoch: 19\n",
      "Epoch 14: Loss 0.22716244774483488\n",
      "Epoch: 15\n",
      "Epoch 19: Loss 0.22707123149840672\n",
      "Epoch: 20\n",
      "Epoch 15: Loss 0.22714376364593894\n",
      "Epoch: 16\n",
      "Epoch 83: Loss 0.9274755596441182\n",
      "Epoch: 84\n",
      "Epoch 20: Loss 0.22705362234067422\n",
      "Epoch: 21\n",
      "Epoch 16: Loss 0.2271253055218498\n",
      "Epoch: 17\n",
      "Epoch 84: Loss 0.9274755596441182\n",
      "Epoch: 85\n",
      "Epoch 17: Loss 0.22710706776944375\n",
      "Epoch: 18\n",
      "Epoch 21: Loss 0.22703621229828527\n",
      "Epoch: 22\n",
      "Epoch 18: Loss 0.22708904488939222\n",
      "Epoch: 19\n",
      "Epoch 22: Loss 0.2270189964003377\n",
      "Epoch: 23\n",
      "Epoch 85: Loss 0.9274755596441182\n",
      "Epoch: 86\n",
      "Epoch 19: Loss 0.22707123149840672\n",
      "Epoch: 20\n",
      "Epoch 23: Loss 0.22700196983068294\n",
      "Epoch: 24\n",
      "Epoch 86: Loss 0.9274755596441182\n",
      "Epoch: 87\n",
      "Epoch 20: Loss 0.22705362234067422\n",
      "Epoch: 21\n",
      "Epoch 24: Loss 0.22698512793405087\n",
      "Epoch: 25\n",
      "Epoch 21: Loss 0.22703621229828527\n",
      "Epoch: 22\n",
      "Epoch 25: Loss 0.22696846622078698\n",
      "Epoch: 26\n",
      "Epoch 87: Loss 0.9274755596441182\n",
      "Epoch: 88\n",
      "Epoch 22: Loss 0.2270189964003377\n",
      "Epoch: 23\n",
      "Epoch 26: Loss 0.22695198036998598\n",
      "Epoch: 27\n",
      "Epoch 88: Loss 0.9274755596441182\n",
      "Epoch: 89\n",
      "Epoch 23: Loss 0.22700196983068294\n",
      "Epoch: 24\n",
      "Epoch 27: Loss 0.22693566623123268\n",
      "Epoch: 28\n",
      "Epoch 24: Loss 0.22698512793405087\n",
      "Epoch: 25\n",
      "Epoch 89: Loss 0.9274755596441182\n",
      "Epoch: 90\n",
      "Epoch 28: Loss 0.22691951982498945\n",
      "Epoch: 29\n",
      "Epoch 25: Loss 0.22696846622078698\n",
      "Epoch: 26\n",
      "Epoch 29: Loss 0.2269035373417659\n",
      "Epoch: 30\n",
      "Epoch 90: Loss 0.9274755596441182\n",
      "Epoch: 91\n",
      "Epoch 26: Loss 0.22695198036998598\n",
      "Epoch: 27\n",
      "Epoch 30: Loss 0.22688771514014175\n",
      "Epoch: 31\n",
      "Epoch 27: Loss 0.22693566623123268\n",
      "Epoch: 28\n",
      "Epoch 31: Loss 0.22687204974389616\n",
      "Epoch: 32\n",
      "Epoch 91: Loss 0.9274755596441182\n",
      "Epoch: 92\n",
      "Epoch 28: Loss 0.22691951982498945\n",
      "Epoch: 29\n",
      "Epoch 32: Loss 0.22685653783825205\n",
      "Epoch: 33\n",
      "Epoch 92: Loss 0.9274755596441182\n",
      "Epoch: 93\n",
      "Epoch 29: Loss 0.2269035373417659\n",
      "Epoch: 30\n",
      "Epoch 33: Loss 0.22684117626542555\n",
      "Epoch: 34\n",
      "Epoch 30: Loss 0.22688771514014175\n",
      "Epoch: 31\n",
      "Epoch 34: Loss 0.22682596201965294\n",
      "Epoch: 35\n",
      "Epoch 93: Loss 0.9274755596441182\n",
      "Epoch: 94\n",
      "Epoch 31: Loss 0.22687204974389616\n",
      "Epoch: 32\n",
      "Epoch 35: Loss 0.22681089224174109\n",
      "Epoch: 36\n",
      "Epoch 94: Loss 0.9274755596441182\n",
      "Epoch: 95\n",
      "Epoch 32: Loss 0.22685653783825205\n",
      "Epoch: 33\n",
      "Epoch 36: Loss 0.2267959642131965\n",
      "Epoch: 37\n",
      "Epoch 33: Loss 0.22684117626542555\n",
      "Epoch: 34\n",
      "Epoch 95: Loss 0.9274755596441182\n",
      "Epoch: 96\n",
      "Epoch 37: Loss 0.22678117535022874\n",
      "Epoch: 38\n",
      "Epoch 34: Loss 0.22682596201965294\n",
      "Epoch: 35\n",
      "Epoch 38: Loss 0.22676652319742874\n",
      "Epoch: 39\n",
      "Epoch 96: Loss 0.9274755596441182\n",
      "Epoch: 97\n",
      "Epoch 35: Loss 0.22681089224174109\n",
      "Epoch: 36\n",
      "Epoch 39: Loss 0.22675200542146265\n",
      "Epoch: 40\n",
      "Epoch 36: Loss 0.2267959642131965\n",
      "Epoch: 37\n",
      "Epoch 97: Loss 0.9274755596441182\n",
      "Epoch: 98\n",
      "Epoch 40: Loss 0.2267376198046028\n",
      "Epoch: 41\n",
      "Epoch 37: Loss 0.22678117535022874\n",
      "Epoch: 38\n",
      "Epoch 41: Loss 0.2267233642383319\n",
      "Epoch: 42\n",
      "Epoch 98: Loss 0.9274755596441182\n",
      "Epoch: 99\n",
      "Epoch 38: Loss 0.22676652319742874\n",
      "Epoch: 39\n",
      "Epoch 42: Loss 0.22670923671694876\n",
      "Epoch: 43\n",
      "Epoch 39: Loss 0.22675200542146265\n",
      "Epoch: 40\n",
      "Epoch 99: Loss 0.9274755596441182\n",
      "Epoch 43: Loss 0.22669523533124847\n",
      "Epoch: 44\n",
      "Epoch 40: Loss 0.2267376198046028\n",
      "Epoch: 41\n",
      "Epoch 44: Loss 0.22668135826231303\n",
      "Epoch: 45\n",
      "Epoch 41: Loss 0.2267233642383319\n",
      "Epoch: 42\n",
      "Epoch 42: Loss 0.22670923671694876\n",
      "Epoch: 43\n",
      "Epoch 45: Loss 0.2266676037754394\n",
      "Epoch: 46\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 694 }, activation: ReLU }, LayerShape { layer_type: Dense { input_size: 694, output_size: 196 }, activation: ReLU }, LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Epoch 43: Loss 0.22669523533124847\n",
      "Epoch: 44\n",
      "Epoch 46: Loss 0.22665397021415645\n",
      "Epoch: 47\n",
      "Epoch 44: Loss 0.22668135826231303\n",
      "Epoch: 45\n",
      "Epoch 47: Loss 0.22664045599453006\n",
      "Epoch: 48\n",
      "Epoch 45: Loss 0.2266676037754394\n",
      "Epoch: 46\n",
      "Epoch 48: Loss 0.22662705959953733\n",
      "Epoch: 49\n",
      "Epoch 46: Loss 0.22665397021415645\n",
      "Epoch: 47\n",
      "Epoch 49: Loss 0.22661377957370407\n",
      "Epoch: 50\n",
      "Epoch 47: Loss 0.22664045599453006\n",
      "Epoch: 48\n",
      "Epoch 50: Loss 0.22660061451796598\n",
      "Epoch: 51\n",
      "Epoch 48: Loss 0.22662705959953733\n",
      "Epoch: 49\n",
      "Epoch 51: Loss 0.2265875630847104\n",
      "Epoch: 52\n",
      "Epoch 49: Loss 0.22661377957370407\n",
      "Epoch: 50\n",
      "Epoch 52: Loss 0.2265746239731025\n",
      "Epoch: 53\n",
      "Epoch 50: Loss 0.22660061451796598\n",
      "Epoch: 51\n",
      "Epoch 53: Loss 0.22656179592465994\n",
      "Epoch: 54\n",
      "Epoch 51: Loss 0.2265875630847104\n",
      "Epoch: 52\n",
      "Epoch 54: Loss 0.22654907771905397\n",
      "Epoch: 55\n",
      "Epoch 52: Loss 0.2265746239731025\n",
      "Epoch: 53\n",
      "Epoch 55: Loss 0.2265364681701905\n",
      "Epoch: 56\n",
      "Epoch 53: Loss 0.22656179592465994\n",
      "Epoch: 54\n",
      "Epoch 56: Loss 0.2265239661225533\n",
      "Epoch: 57\n",
      "Epoch 54: Loss 0.22654907771905397\n",
      "Epoch: 55\n",
      "Epoch 57: Loss 0.22651157044783557\n",
      "Epoch: 58\n",
      "Epoch 55: Loss 0.2265364681701905\n",
      "Epoch: 56\n",
      "Epoch 58: Loss 0.22649928004178857\n",
      "Epoch: 59\n",
      "Epoch 56: Loss 0.2265239661225533\n",
      "Epoch: 57\n",
      "Epoch 57: Loss 0.22651157044783557\n",
      "Epoch: 58\n",
      "Epoch 59: Loss 0.226487093821408\n",
      "Epoch: 60\n",
      "Epoch 58: Loss 0.22649928004178857\n",
      "Epoch: 59\n",
      "Epoch 60: Loss 0.2264750107222679\n",
      "Epoch: 61\n",
      "Epoch 59: Loss 0.226487093821408\n",
      "Epoch: 60\n",
      "Epoch 61: Loss 0.2264630296962446\n",
      "Epoch: 62\n",
      "Epoch 60: Loss 0.2264750107222679\n",
      "Epoch: 61\n",
      "Epoch 62: Loss 0.22645114970933972\n",
      "Epoch: 63\n",
      "Epoch 61: Loss 0.2264630296962446\n",
      "Epoch: 62\n",
      "Epoch 63: Loss 0.22643936973988096\n",
      "Epoch: 64\n",
      "Epoch 62: Loss 0.22645114970933972\n",
      "Epoch: 63\n",
      "Epoch 64: Loss 0.2264276887767948\n",
      "Epoch: 65\n",
      "Epoch 63: Loss 0.22643936973988096\n",
      "Epoch: 64\n",
      "Epoch 65: Loss 0.22641610581817143\n",
      "Epoch: 66\n",
      "Epoch 64: Loss 0.2264276887767948\n",
      "Epoch: 65\n",
      "Epoch 66: Loss 0.2264046198700275\n",
      "Epoch: 67\n",
      "Epoch 65: Loss 0.22641610581817143\n",
      "Epoch: 66\n",
      "Epoch 67: Loss 0.22639322994517413\n",
      "Epoch: 68\n",
      "Epoch 66: Loss 0.2264046198700275\n",
      "Epoch: 67\n",
      "Epoch 68: Loss 0.2263819350623021\n",
      "Epoch: 69\n",
      "Epoch 67: Loss 0.22639322994517413\n",
      "Epoch: 68\n",
      "Epoch 69: Loss 0.22637073424517254\n",
      "Epoch: 70\n",
      "Epoch 68: Loss 0.2263819350623021\n",
      "Epoch: 69\n",
      "Epoch 70: Loss 0.22635962652194913\n",
      "Epoch: 71\n",
      "Epoch 69: Loss 0.22637073424517254\n",
      "Epoch: 70\n",
      "Epoch 71: Loss 0.22634861092465902\n",
      "Epoch: 72\n",
      "Epoch 70: Loss 0.22635962652194913\n",
      "Epoch: 71\n",
      "Epoch 72: Loss 0.22633768648867955\n",
      "Epoch: 73\n",
      "Epoch 71: Loss 0.22634861092465902\n",
      "Epoch: 72\n",
      "Epoch 73: Loss 0.22632685225238505\n",
      "Epoch: 74\n",
      "Epoch 72: Loss 0.22633768648867955\n",
      "Epoch: 73\n",
      "Epoch 74: Loss 0.22631610725682033\n",
      "Epoch: 75\n",
      "Epoch 73: Loss 0.22632685225238505\n",
      "Epoch: 74\n",
      "Epoch 75: Loss 0.2263054505454406\n",
      "Epoch: 76\n",
      "Epoch 74: Loss 0.22631610725682033\n",
      "Epoch: 75\n",
      "Epoch 76: Loss 0.22629488116390362\n",
      "Epoch: 77\n",
      "Epoch 75: Loss 0.2263054505454406\n",
      "Epoch: 76\n",
      "Epoch 77: Loss 0.22628439815987872\n",
      "Epoch: 78\n",
      "Epoch 76: Loss 0.22629488116390362\n",
      "Epoch: 77\n",
      "Epoch 78: Loss 0.2262740005829226\n",
      "Epoch: 79\n",
      "Epoch 77: Loss 0.22628439815987872\n",
      "Epoch: 78\n",
      "Epoch 79: Loss 0.22626368748438322\n",
      "Epoch: 80\n",
      "Epoch 78: Loss 0.2262740005829226\n",
      "Epoch: 79\n",
      "Epoch 80: Loss 0.22625345791726467\n",
      "Epoch: 81\n",
      "Epoch 79: Loss 0.22626368748438322\n",
      "Epoch: 80\n",
      "Epoch 81: Loss 0.22624331093618688\n",
      "Epoch: 82\n",
      "Epoch 80: Loss 0.22625345791726467\n",
      "Epoch: 81\n",
      "Epoch 82: Loss 0.2262332455973182\n",
      "Epoch: 83\n",
      "Epoch 81: Loss 0.22624331093618688\n",
      "Epoch: 82\n",
      "Epoch 83: Loss 0.22622326095835713\n",
      "Epoch: 84\n",
      "Epoch 82: Loss 0.2262332455973182\n",
      "Epoch: 83\n",
      "Epoch 84: Loss 0.22621335607848916\n",
      "Epoch: 85\n",
      "Epoch 83: Loss 0.22622326095835713\n",
      "Epoch: 84\n",
      "Epoch 85: Loss 0.2262035300183739\n",
      "Epoch: 86\n",
      "Epoch 84: Loss 0.22621335607848916\n",
      "Epoch: 85\n",
      "Epoch 86: Loss 0.22619378184016625\n",
      "Epoch: 87\n",
      "Epoch 85: Loss 0.2262035300183739\n",
      "Epoch: 86\n",
      "Epoch 87: Loss 0.2261841106075483\n",
      "Epoch: 88\n",
      "Epoch 86: Loss 0.22619378184016625\n",
      "Epoch: 87\n",
      "Epoch 88: Loss 0.22617451538574943\n",
      "Epoch: 89\n",
      "Epoch 87: Loss 0.2261841106075483\n",
      "Epoch: 88\n",
      "Epoch 89: Loss 0.2261649952416156\n",
      "Epoch: 90\n",
      "Epoch 88: Loss 0.22617451538574943\n",
      "Epoch: 89\n",
      "Epoch 89: Loss 0.2261649952416156\n",
      "Epoch: 90\n",
      "Epoch 90: Loss 0.22615554924370215\n",
      "Epoch: 91\n",
      "Epoch 90: Loss 0.22615554924370215\n",
      "Epoch: 91\n",
      "Epoch 91: Loss 0.22614617646238083\n",
      "Epoch: 92\n",
      "Epoch 91: Loss 0.22614617646238083\n",
      "Epoch: 92\n",
      "Epoch 92: Loss 0.2261368759699354\n",
      "Epoch: 93\n",
      "Epoch 92: Loss 0.2261368759699354\n",
      "Epoch: 93\n",
      "Epoch 93: Loss 0.22612764684077907\n",
      "Epoch: 94\n",
      "Epoch 93: Loss 0.22612764684077907\n",
      "Epoch: 94\n",
      "Epoch 94: Loss 0.22611848815154098\n",
      "Epoch: 95\n",
      "Epoch 94: Loss 0.22611848815154098\n",
      "Epoch: 95\n",
      "Epoch 95: Loss 0.22610939898140514\n",
      "Epoch: 96\n",
      "Epoch 95: Loss 0.22610939898140514\n",
      "Epoch: 96\n",
      "Epoch 96: Loss 0.2261003784122206\n",
      "Epoch: 97\n",
      "Epoch 96: Loss 0.2261003784122206\n",
      "Epoch: 97\n",
      "Epoch 97: Loss 0.22609142552883466\n",
      "Epoch: 98\n",
      "Epoch 97: Loss 0.22609142552883466\n",
      "Epoch: 98\n",
      "Epoch 98: Loss 0.22608253941939604\n",
      "Epoch: 99\n",
      "Epoch 98: Loss 0.22608253941939604\n",
      "Epoch: 99\n",
      "Epoch 99: Loss 0.22607371917565572\n",
      "Epoch 99: Loss 0.22607371917565572\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 254 }, activation: ReLU }, LayerShape { layer_type: Dense { input_size: 254, output_size: 196 }, activation: ReLU }, LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Inputs: 59999 x 196\n",
      "Targets: 59999 x 10\n",
      "Training neural network with shape: NeuralNetworkShape { layers: [LayerShape { layer_type: Dense { input_size: 196, output_size: 10 }, activation: Sigmoid }] }\n",
      "Epoch: 0\n",
      "Epoch 0: Loss 0.2274510580259939\n",
      "Epoch: 1\n",
      "Epoch 1: Loss 0.2274285827091618\n",
      "Epoch: 2\n",
      "Epoch 2: Loss 0.22740641955216312\n",
      "Epoch: 3\n",
      "Epoch 3: Loss 0.22738456161369386\n",
      "Epoch: 4\n",
      "Epoch 4: Loss 0.22736300219477537\n",
      "Epoch: 5\n",
      "Epoch 5: Loss 0.22734173478888087\n",
      "Epoch: 6\n",
      "Epoch 6: Loss 0.2273207530386601\n",
      "Epoch: 7\n",
      "Epoch 7: Loss 0.22730005070032233\n",
      "Epoch: 8\n",
      "Epoch 8: Loss 0.22727962161575307\n",
      "Epoch: 9\n",
      "Epoch 1: Loss 0.9274755596441182\n",
      "Epoch: 2\n",
      "Epoch 9: Loss 0.2272594596924186\n",
      "Epoch: 10\n",
      "Epoch 10: Loss 0.22723955889049963\n",
      "Epoch: 11\n",
      "Epoch 11: Loss 0.22721991321665383\n",
      "Epoch: 12\n",
      "Epoch 12: Loss 0.22720051672336836\n",
      "Epoch: 13\n",
      "Epoch 13: Loss 0.2271813635129094\n",
      "Epoch: 14\n",
      "Epoch 14: Loss 0.22716244774483488\n",
      "Epoch: 15\n",
      "Epoch 15: Loss 0.22714376364593894\n",
      "Epoch: 16\n",
      "Epoch 16: Loss 0.2271253055218498\n",
      "Epoch: 17\n",
      "Epoch 17: Loss 0.22710706776944375\n",
      "Epoch: 18\n",
      "Epoch 18: Loss 0.22708904488939222\n",
      "Epoch: 19\n",
      "Epoch 19: Loss 0.22707123149840672\n",
      "Epoch: 20\n",
      "Epoch 20: Loss 0.22705362234067422\n",
      "Epoch: 21\n",
      "Epoch 21: Loss 0.22703621229828527\n",
      "Epoch: 22\n",
      "Epoch 22: Loss 0.2270189964003377\n",
      "Epoch: 23\n",
      "Epoch 23: Loss 0.22700196983068294\n",
      "Epoch: 24\n",
      "Epoch 24: Loss 0.22698512793405087\n",
      "Epoch: 25\n",
      "Epoch 25: Loss 0.22696846622078698\n",
      "Epoch: 26\n",
      "Epoch 26: Loss 0.22695198036998598\n",
      "Epoch: 27\n",
      "Epoch 27: Loss 0.22693566623123268\n",
      "Epoch: 28\n",
      "Epoch 28: Loss 0.22691951982498945\n",
      "Epoch: 29\n",
      "Epoch 29: Loss 0.2269035373417659\n",
      "Epoch: 30\n",
      "Epoch 30: Loss 0.22688771514014175\n",
      "Epoch: 31\n",
      "Epoch 31: Loss 0.22687204974389616\n",
      "Epoch: 32\n",
      "Epoch 32: Loss 0.22685653783825205\n",
      "Epoch: 33\n",
      "Epoch 33: Loss 0.22684117626542555\n",
      "Epoch: 34\n",
      "Epoch 34: Loss 0.22682596201965294\n",
      "Epoch: 35\n",
      "Epoch 35: Loss 0.22681089224174109\n",
      "Epoch: 36\n",
      "Epoch 36: Loss 0.2267959642131965\n",
      "Epoch: 37\n",
      "Epoch 37: Loss 0.22678117535022874\n",
      "Epoch: 38\n",
      "Epoch 38: Loss 0.22676652319742874\n",
      "Epoch: 39\n",
      "Epoch 39: Loss 0.22675200542146265\n",
      "Epoch: 40\n",
      "Epoch 40: Loss 0.2267376198046028\n",
      "Epoch: 41\n",
      "Epoch 41: Loss 0.2267233642383319\n",
      "Epoch: 42\n",
      "Epoch 42: Loss 0.22670923671694876\n",
      "Epoch: 43\n",
      "Epoch 43: Loss 0.22669523533124847\n",
      "Epoch: 44\n",
      "Epoch 44: Loss 0.22668135826231303\n",
      "Epoch: 45\n",
      "Epoch 45: Loss 0.2266676037754394\n",
      "Epoch: 46\n",
      "Epoch 46: Loss 0.22665397021415645\n",
      "Epoch: 47\n",
      "Epoch 47: Loss 0.22664045599453006\n",
      "Epoch: 48\n",
      "Epoch 48: Loss 0.22662705959953733\n",
      "Epoch: 49\n",
      "Epoch 49: Loss 0.22661377957370407\n",
      "Epoch: 50\n",
      "Epoch 0: Loss 0.9273485159084759\n",
      "Epoch: 1\n",
      "Epoch 50: Loss 0.22660061451796598\n",
      "Epoch: 51\n",
      "Epoch 51: Loss 0.2265875630847104\n",
      "Epoch: 52\n",
      "Epoch 52: Loss 0.2265746239731025\n",
      "Epoch: 53\n",
      "Epoch 53: Loss 0.22656179592465994\n",
      "Epoch: 54\n",
      "Epoch 54: Loss 0.22654907771905397\n",
      "Epoch: 55\n",
      "Epoch 55: Loss 0.2265364681701905\n",
      "Epoch: 56\n",
      "Epoch 56: Loss 0.2265239661225533\n",
      "Epoch: 57\n",
      "Epoch 57: Loss 0.22651157044783557\n",
      "Epoch: 58\n",
      "Epoch 58: Loss 0.22649928004178857\n",
      "Epoch: 59\n",
      "Epoch 59: Loss 0.226487093821408\n",
      "Epoch: 60\n",
      "Epoch 60: Loss 0.2264750107222679\n",
      "Epoch: 61\n",
      "Epoch 61: Loss 0.2264630296962446\n",
      "Epoch: 62\n",
      "Epoch 62: Loss 0.22645114970933972\n",
      "Epoch: 63\n",
      "Epoch 63: Loss 0.22643936973988096\n",
      "Epoch: 64\n",
      "Epoch 64: Loss 0.2264276887767948\n",
      "Epoch: 65\n",
      "Epoch 65: Loss 0.22641610581817143\n",
      "Epoch: 66\n",
      "Epoch 66: Loss 0.2264046198700275\n",
      "Epoch: 67\n",
      "Epoch 67: Loss 0.22639322994517413\n",
      "Epoch: 68\n",
      "Epoch 68: Loss 0.2263819350623021\n",
      "Epoch: 69\n",
      "Epoch 69: Loss 0.22637073424517254\n",
      "Epoch: 70\n",
      "Epoch 70: Loss 0.22635962652194913\n",
      "Epoch: 71\n",
      "Epoch 71: Loss 0.22634861092465902\n",
      "Epoch: 72\n",
      "Epoch 72: Loss 0.22633768648867955\n",
      "Epoch: 73\n",
      "Epoch 73: Loss 0.22632685225238505\n",
      "Epoch: 74\n",
      "Epoch 74: Loss 0.22631610725682033\n",
      "Epoch: 75\n",
      "Epoch 75: Loss 0.2263054505454406\n",
      "Epoch: 76\n",
      "Epoch 76: Loss 0.22629488116390362\n",
      "Epoch: 77\n",
      "Epoch 77: Loss 0.22628439815987872\n",
      "Epoch: 78\n",
      "Epoch 78: Loss 0.2262740005829226\n",
      "Epoch: 79\n",
      "Epoch 0: Loss 0.9273970015866785\n",
      "Epoch: 1\n",
      "Epoch 79: Loss 0.22626368748438322\n",
      "Epoch: 80\n",
      "Epoch 80: Loss 0.22625345791726467\n",
      "Epoch: 81\n",
      "Epoch 81: Loss 0.22624331093618688\n",
      "Epoch: 82\n",
      "Epoch 82: Loss 0.2262332455973182\n",
      "Epoch: 83\n",
      "Epoch 83: Loss 0.22622326095835713\n",
      "Epoch: 84\n",
      "Epoch 84: Loss 0.22621335607848916\n",
      "Epoch: 85\n",
      "Epoch 85: Loss 0.2262035300183739\n",
      "Epoch: 86\n",
      "Epoch 86: Loss 0.22619378184016625\n",
      "Epoch: 87\n",
      "Epoch 87: Loss 0.2261841106075483\n",
      "Epoch: 88\n",
      "Epoch 88: Loss 0.22617451538574943\n",
      "Epoch: 89\n",
      "Epoch 89: Loss 0.2261649952416156\n",
      "Epoch: 90\n",
      "Epoch 90: Loss 0.22615554924370215\n",
      "Epoch: 91\n",
      "Epoch 91: Loss 0.22614617646238083\n",
      "Epoch: 92\n",
      "Epoch 92: Loss 0.2261368759699354\n",
      "Epoch: 93\n",
      "Epoch 93: Loss 0.22612764684077907\n",
      "Epoch: 94\n",
      "Epoch 94: Loss 0.22611848815154098\n",
      "Epoch: 95\n",
      "Epoch 95: Loss 0.22610939898140514\n",
      "Epoch: 96\n",
      "Epoch 96: Loss 0.2261003784122206\n",
      "Epoch: 97\n",
      "Epoch 97: Loss 0.22609142552883466\n",
      "Epoch: 98\n",
      "Epoch 1: Loss 0.9274755596441182\n",
      "Epoch: 2\n",
      "Epoch 98: Loss 0.22608253941939604\n",
      "Epoch: 99\n",
      "Epoch 99: Loss 0.22607371917565572\n",
      "Epoch 2: Loss 0.9274755596441182\n",
      "Epoch: 3\n",
      "Epoch 2: Loss 0.9274755596441182\n",
      "Epoch: 3\n",
      "Epoch 3: Loss 0.9274755596441182\n",
      "Epoch: 4\n",
      "Epoch 1: Loss 0.9274755596441182\n",
      "Epoch: 2\n",
      "Epoch 4: Loss 0.9274755596441182\n",
      "Epoch: 5\n",
      "Epoch 3: Loss 0.9274755596441182\n",
      "Epoch: 4\n",
      "Epoch 5: Loss 0.9274755596441182\n",
      "Epoch: 6\n",
      "Epoch 6: Loss 0.9274755596441182\n",
      "Epoch: 7\n",
      "Epoch 2: Loss 0.9274755596441182\n",
      "Epoch: 3\n",
      "Epoch 4: Loss 0.9274755596441182\n",
      "Epoch: 5\n",
      "Epoch 7: Loss 0.9274755596441182\n",
      "Epoch: 8\n",
      "Epoch 8: Loss 0.9274755596441182\n",
      "Epoch: 9\n",
      "Epoch 9: Loss 0.9274755596441182\n",
      "Epoch: 10\n",
      "Epoch 5: Loss 0.9274755596441182\n",
      "Epoch: 6\n",
      "Epoch 3: Loss 0.9274755596441182\n",
      "Epoch: 4\n",
      "Epoch 10: Loss 0.9274755596441182\n",
      "Epoch: 11\n",
      "Epoch 11: Loss 0.9274755596441182\n",
      "Epoch: 12\n",
      "Epoch 6: Loss 0.9274755596441182\n",
      "Epoch: 7\n",
      "Epoch 12: Loss 0.9274755596441182\n",
      "Epoch: 13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     env[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRUST_BACKTRACE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Run the command\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training completed successfully and saved in:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_directory)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/subprocess.py:1201\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/subprocess.py:1264\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/subprocess.py:2053\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 2053\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/subprocess.py:2011\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2011\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mwaitpid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid, wait_flags)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   2013\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   2015\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   2016\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define parameters for `nn_generator`\n",
    "model_directory = \"./trained_model\"\n",
    "# create the trained model directory\n",
    "import os\n",
    "os.makedirs(model_directory, exist_ok=True)\n",
    "\n",
    "input_file = \"input.csv\"\n",
    "target_file = \"target.csv\"\n",
    "training_verification_ratio = 0.7\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "tolerance = 0.1\n",
    "batch_size = 32\n",
    "num_generations = 100\n",
    "log_level = 1\n",
    "population_size = 4\n",
    "num_offsprings = 10\n",
    "\n",
    "# Create the command for training the neural network\n",
    "command = [\n",
    "    \"./nn_generator\",\n",
    "    \"--model-directory\", model_directory,\n",
    "    \"--input-file\", input_file,\n",
    "    \"--target-file\", target_file,\n",
    "    \"--training-verification-ratio\", str(training_verification_ratio),\n",
    "    \"--learning-rate\", str(learning_rate),\n",
    "    \"--epochs\", str(epochs),\n",
    "    \"--tolerance\", str(tolerance),\n",
    "    \"--batch-size\", str(batch_size),\n",
    "    \"--num-generations\", str(num_generations),\n",
    "    \"--log-level\", str(log_level),\n",
    "    \"--population-size\", str(population_size),\n",
    "    \"--num-offsprings\", str(num_offsprings)\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "try:\n",
    "    #subprocess.run(command, check=True)\n",
    "    # Create a copy of the current environment variables and add RUST_BACKTRACE=1\n",
    "    env = os.environ.copy()\n",
    "    env[\"RUST_BACKTRACE\"] = \"1\"\n",
    "\n",
    "    # Run the command\n",
    "    subprocess.run(command, check=True, env=env)\n",
    "    print(\"Model training completed successfully and saved in:\", model_directory)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error occurred during model training:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827f8e5",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5477d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate on test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(x_test_flat, y_test_onehot)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "loss, accuracy = model.evaluate(x_test_flat, y_test_onehot)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
