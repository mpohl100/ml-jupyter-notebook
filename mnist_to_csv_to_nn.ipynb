{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5baa1a4e",
   "metadata": {},
   "source": [
    "# MNIST Dataset Processing and Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a41b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 10:38:33.185467: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-07 10:38:33.189243: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-07 10:38:33.200164: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733567913.220071    1063 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733567913.225277    1063 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-07 10:38:33.247494: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7a49b",
   "metadata": {},
   "source": [
    "## Step 1: Download and Process MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a423b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Resize images to 16x16 (256 pixels)\n",
    "x_train = x_train[:, ::2, ::2]  # Downsample by skipping every other pixel\n",
    "x_test = x_test[:, ::2, ::2]\n",
    "\n",
    "# Normalize pixel values to range [0.0, 1.0] (0.0 for white, 1.0 for black)\n",
    "x_train = 1 - x_train / 255.0\n",
    "x_test = 1 - x_test / 255.0\n",
    "\n",
    "# Flatten images into 256-pixel vectors\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train, 10)\n",
    "y_test_onehot = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed3dd7",
   "metadata": {},
   "source": [
    "## Step 2: Save Processed Data to CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acefa21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved input.csv and target.csv.\n"
     ]
    }
   ],
   "source": [
    "# Save input data to input.csv\n",
    "input_df = pd.DataFrame(x_train_flat)\n",
    "input_df.to_csv(\"input.csv\", index=False)\n",
    "\n",
    "# Save target data to target.csv\n",
    "target_df = pd.DataFrame(y_train_onehot)\n",
    "target_df.to_csv(\"target.csv\", index=False)\n",
    "\n",
    "print(\"Saved input.csv and target.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f708ac4e",
   "metadata": {},
   "source": [
    "## Step 3: Create a Neural Network using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb6f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred during model training: Command '['./nn_generator', '--model-directory', './trained_model', '--shape-file', './shape.txt', '--input-file', 'input.csv', '--target-file', 'target.csv', '--training-verification-ratio', '0.7', '--learning-rate', '0.01', '--epochs', '100', '--tolerance', '0.1', '--num-generations', '100', '--log-level', '1', '--population-size', '4', '--num-offsprings', '10']' returned non-zero exit status 101.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread 'main' panicked at src/neural/nn/shape.rs:90:42:\n",
      "called `Result::unwrap()` on an `Err` value: Os { code: 2, kind: NotFound, message: \"No such file or directory\" }\n",
      "note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define parameters for `nn_generator`\n",
    "model_directory = \"./trained_model\"\n",
    "# create the trained model directory\n",
    "import os\n",
    "os.makedirs(model_directory, exist_ok=True)\n",
    "\n",
    "input_file = \"input.csv\"\n",
    "target_file = \"target.csv\"\n",
    "training_verification_ratio = 0.7\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "tolerance = 0.1\n",
    "num_generations = 100\n",
    "log_level = 1\n",
    "population_size = 4\n",
    "num_offsprings = 10\n",
    "\n",
    "# Create the command for training the neural network\n",
    "command = [\n",
    "    \"./nn_generator\",\n",
    "    \"--model-directory\", model_directory,\n",
    "    \"--input-file\", input_file,\n",
    "    \"--target-file\", target_file,\n",
    "    \"--training-verification-ratio\", str(training_verification_ratio),\n",
    "    \"--learning-rate\", str(learning_rate),\n",
    "    \"--epochs\", str(epochs),\n",
    "    \"--tolerance\", str(tolerance),\n",
    "    \"--num-generations\", str(num_generations),\n",
    "    \"--log-level\", str(log_level),\n",
    "    \"--population-size\", str(population_size),\n",
    "    \"--num-offsprings\", str(num_offsprings)\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "try:\n",
    "    subprocess.run(command, check=True)\n",
    "    print(\"Model training completed successfully and saved in:\", model_directory)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error occurred during model training:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827f8e5",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5477d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate on test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(x_test_flat, y_test_onehot)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "loss, accuracy = model.evaluate(x_test_flat, y_test_onehot)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
